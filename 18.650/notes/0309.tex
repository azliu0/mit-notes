\section{March 9, 2023}

\subsection{Empirical CDF}

\begin{definition}
\deflabel

The \ac{empirical cdf} of a sample $X_1, \hdots, X_n$ is defined as 
\[F_n(t) = \frac{1}{t}\sum_{i=1}^n\mathbbm{1}(X_i\leq t) = \frac{\#\{i=1, \hdots, n : X_i\leq t\}}{n}.\]
\end{definition}

Let $Z_i = \mathbbm{1}(X_i\leq x)$. $\PP[Z_i=1] = \PP[X_i\leq x] = F(x)$, so $Z_i\sim \Bern(F(x))$. This implies that $F_n(t)$ is an unbiased estimator for $F(t)$: 
\begin{align*}
\EE[F_n(x)] &= \frac{1}{n}\sum_{i=1}^n\EE[\mathbbm{1}(x_i\leq x)]\\
&= \frac{1}{n}(nF(x)) = F(x),
\end{align*}
By the strong law of large numbers, $F_n(t)$ is also a consistent estimator: 
\[F_n(t)\cas F(t).\]
Also, the central limit theorem holds: 
\[\sqrt{n}(F_n(x) - F(x))\cd \Norm(0,F(x)(1-F(x))).\]

\begin{theorem}
\thmlabelname{Glivenko-Cantelli's Theorem}
\[\sup_{t\in \RR}\vert F_n(t) - F(t)\vert\cas 0.\]
\end{theorem}
This theorem is also known as the \ac{Fundamental Theorem of Statistics}. The next theorem is a generalization of the central limit theorem: 
\begin{theorem}
\thmlabelname{Donsker's Theorem}

If $F$ is continuous, then 
\[\sqrt{n}\sup_{t\in \RR}\vert F_n(t) - F(t)\vert \cd \sup_{0\leq t\leq 1}\vert \BB(t)\vert,\]
where $\BB$ is a brownian bridge on $[0,1]$.
\end{theorem}

A brownian bridge on $[0,1]$ is a function modelling Brownian motion which starts and ends at the same point $(0)$. 

\subsection{Kolmogorov-Smirnov Test}

Let $X_1, \hdots, X_n$ be i.i.d. real random variables with unknown cdf $F$ and let $F^0$ be a continuous cdf. Let
\begin{align*}
    H_0 : F=F_0\\
    H_1 : F\neq F_0
\end{align*}
Note that the null hypothesis means that $F(x) = F_0(x)\forall x\in \RR$. Let 
\[T_n = \sup_{x}\vert F_n(x) - F_0(x)\vert.\]
By Donsker's Theorem, given $H_0$, 
\[\sqrt{n}T_n\cd \sup_{0\leq t\leq 1}\vert \BB(t)\vert=Z.\]
Therefore, a level $\alpha$ test is given by 
\[\Psi = \mathbbm{1}(T_n > \tilde{q}_{1-\alpha}/\sqrt{n}),\]
where $\tilde{q}_{1-\alpha}$ is the $(1-\alpha)$ quantile of $Z$. The $p$-value of this test is given by $\PP[Z > T_n^{obs}]$. This test is called the \ac{Kolmogorov-Smirnov Test}. 

\subsection{Computing values for KS}

\textbf{Computing the test statistic:}

Let $X_{(1)}, \hdots, X_{(n)}$ be the reordered sample. Note that $F_0$ is non decreasing, while $F_n$ is piecewise constant, such that $F_n$ jumps from $(i-1)/n$ to $i/n$ at $X_{(i)}$. Thus, 
\[T_n = \max_{i=1,\hdots, n}\left\{\left\vert\frac{i-1}{n}-F_0(X_{(i)})\right\vert, \left\vert\frac{i}{n}-F_0(X_{(i)})\right\vert\right\}.\]

% $T_n$ is said to be \ac{pivotal}, since, given that the null hypothesis is true, the distribution of $T_n$ does not depend on the distribution of the $X_i$s. 

\noindent\textbf{Computing the quantiles: }

Let $U_i\sim F_0(X_i)$. If $H_0$ is true, then $U_1, \hdots, U_n\sim \Unif[0,1]$. (Intuitively, since we are drawing from the same distribution, the cdf probabilities should be distributed uniformly). In this case, $T_n=\sup_{0\leq x\leq 1}\vert G_n(x)-x\vert$, where $G_n$ is the emprical cdf of $U_1,\hdots, U_n$. Note that $G_n$ does not depend on the distribution of the $X_i$s (as long as the null hypothesis is true). Because of this property, we say that $T_n$ is a \ac{pivotal statistic}.

To compute the quantiles, first sample $k$ batches of $(U_1, \hdots U_n)\sim \Unif[0,1]$. Then, compute the test statistics $(\tilde{T}_n^{1}, \hdots, \tilde{T}_n^{k})$. Estimate the $(1-\alpha)$ quantile $q_{1-\alpha}^{(n)}$ by taking the sample $(1-\alpha)$ quantile $\hat{q}_{1-\alpha}^{(n,k)}$ of our test statistics. The way that sample quantile is defined is by turning our $k$ test statistics into a histogram, and then taking the $(1-\alpha)$ quantile of that histogram. 

\subsection{Kolmogorov-Lilliefors Test}

To test if $X$ is gaussian, we would need to know the exact parameters of the gaussian in order to perform Kolmogorov-Smirnov (since we assumed we knew $F_0$ precisely). So, if we want to test if it is gaussian in general, we can instead use the test statistic 
\[\hat{T}_n = \sup_{t\in \RR}\vert F_n(t) - \Phi_{\hat{\mu}, \hat{\sigma}^2}(t)\vert,\]
where $\hat{\mu} = \ov{X}_n$ and $\hat{\sigma}^2 = S_n^2$. This is the \ac{Kolmogorov-Lilliefors Test}. 

\subsection{Linear Regression}

Given two random variables $(X,Y)$ we want to compute a regression function to predict $Y$ from $X$. Our samples are $(X_i, Y_i)$ i.i.d. from an unkonwn joint distribution $\PP$. Two ways that this distribution could be described:
\begin{itemize}
    \item a joint pdf $h(x,y)$
    \item the marginal density $h(x) = \int h(x,y)\ddd y$, and a conditional density $h(y|x) = h(x,y)/h(x)$. 
\end{itemize}

So, we may theoretically extract some information about our regression in the following ways:
\begin{itemize}
    \item $f(x) = \EE[Y | X=x] = \int yh(y|x)\ddd y$. The regression function $f(x)$ is often hard to compute in practice.
    \item Conditional median: 
    \[\int_{-\infty}^m h(y|x)\ddd y = \frac{1}{2}.\]
    \item We can also extract conditional quantiles, using the same idea as above. 
\end{itemize}

In linear regression specifically, we assume $\EE[Y | X=x] = a+bx$. The theoretical linear regression is given by 
\[(a^*, b^*) = \argmin_{(a,b)\in \RR^2}\EE[(Y-a-bX)^2].\]
Setting the partial derivatives equal to $0$, 
\[(a^*, b^*) = \left(\EE[Y] - b^*\EE[X], \frac{\cov{(X, Y)}}{\var[X]}\right).\]

The random variable $\varepsilon = Y - (a^*+b^*X)$ is called \ac{noise}, and satisfies $\EE[\varepsilon] = 0$, $\cov{(X, \varepsilon)} = 0$. 
