\section{April 27}

Review session. 

\subsection{Exponential Distributions}

\[\PP_{\eta}(x) = h(x)\exp(\eta^Tt(x) - a(\eta))\ddd x,\]
where $a(\eta)$ is a normalizing factor so that $\int \PP_{\eta}(x)\ddd x = 1$.

\begin{example}
\exlabel

Show that $\partial a/ (\partial \eta_i) = \EE[t_i(x)]$. 
\end{example}
Given 
\[1 = \int h(x) \exp(\eta^Tt(x) - a(\eta))\ddd x,\]
we know 
\[\exp(a(\eta)) = \int h(x) \exp(\eta_1t_1(x) + \eta_2t_2(x) + \hdots + \eta_nt_n(x))\ddd x.\]
Differentiate both sides with respect to $\eta_i$:
\[\partial a_i/\partial \eta_i\cdot (\exp(a(\eta))) = \int t_i(x)h(x)\exp(\eta^Tt(x))\ddd x,\]
which implies 
\[\partial a_i/\partial \eta_i= \int t_i(x) \PP_{\eta}(x)\ddd x = \EE[t_i(x)].\]

\begin{theorem}
\thmlabel

More generally, $\nabla a(\eta) = \EE[t(x)]$ and $\nabla^2 a(\eta) = \cov(t(x))$.
\end{theorem}

\subsection{Bayesian Statistics}

Let $X_1, \hdots, X_n\sim \Exp(\lambda)$.

\begin{itemize}
    \item classical: make an asymptotic statement about $\lambda$, create confidence intervals, etc.
    \item Bayesian: assume $\lambda$ has a distribution, use the data to refine the distribution.
\end{itemize}

Bayesian statistics starts with a \ac{prior} $\pi(\lambda)$, which is the initial distribution that we guess that $\lambda$ has. The model creates a \ac{posterior} $\pi(\lambda|X_1, \hdots, X_n)$, which is the updated distribution after considering the data. 

Bayes rule: posterior is proportional to the prior times the likelihood of the data:
\[\pi(\lambda | X_1, \hdots, X_n) \propto \pi(\lambda)\cdot \PP(X_1, \hdots, X_n | \lambda).\]

Estimators:
\begin{itemize}
    \item MAP (maximum a posterior): the value of $\lambda$ maximizing the posterior
    \item Bayes: the mean of $\pi(\lambda | X_1, \hdots, X_n)$.
\end{itemize}

Note: given a uniform prior, maximizing the posterior is the same as maximizing the likelihood, so the MAP is the same as the MLE.

\begin{example}
\exlabel

Given data $X_1, \hdots, X_n\sim \Pois(\lambda)$ and prior $\lambda\sim \Unif[0,10]$, compute the MAP and Bayes estimators. 
\end{example}
We know 
\[\pi(\lambda|X_1, \hdots, X_n) \propto \pi(\lambda)\PP(X_1, \hdots, X_n|\lambda).\]
The density of $\Pois(\lambda)$ is given by $f_{\lambda}(x) = \lambda^xe^{-\lambda}/x!$, so 
\[\pi(\lambda|X_1, \hdots, X_n) = c\cdot \frac{1}{10}\mathbbm{1}(\lambda \in [0,10])\lambda^{\sum X_i}e^{-\lambda n},\]
for some normalizing constant $c$. From here, asssume $X_1, \hdots, X_n = 0$ to save some computation. First, we compute the constant:
\[1 = \int_{0}^{10}\frac{1}{10}ce^{-\lambda n}\ddd \lambda = \frac{c(1-e^{-10n})}{10n}\implies c = \frac{10n}{1-e^{-10n}},\]
which gives us the posterior: 
\[\pi(\lambda|X_1, \hdots, X_n) = \frac{n}{1-e^{-10n}}e^{-n\lambda}\mathbbm{1}(\lambda\in [0,10]).\]
Now, the Bayes estimator is given by 
\[\hat{\lambda} = \EE[\pi(\lambda|X_1, \hdots, X_n)] = \int_{0}^{10}\frac{n\lambda}{1-e^{-10n}}e^{-n\lambda}\ddd\lambda.\]

To find the MAP estimator, differentiate (we no longer assume the values of $X_i$ since this is an easier computation): 
\begin{align*}
    \frac{\partial}{\partial \lambda}\left(\frac{1}{10}\lambda^{\sum X_i}e^{-nx}\right) = 0\\
    \implies \lambda = \left(\sum X_i\right)/n.
\end{align*}
With consideration to the constraint on $\lambda$, this gives us the MAP estimator $\hat{\lambda} = \min((\sum X_i)/n, 10)$. 

\begin{example}
\exlabel

Compute Jeffrey's prior for $\Pois(\lambda)$ and $\Norm(0,\theta)$.
\end{example}

\begin{itemize}
    \item $\Pois(\lambda)$: For a poisson distribution, 
    \[f_{\lambda}(x) = \frac{\lambda^x}{x!}e^{-\lambda},\]
    so the total log likelihood is
    \[LL(X_i|\lambda) = \sum (X_i\log \lambda - \log x!-\lambda).\]
    Differentiating, we get 
    \[\frac{\partial LL(X_i|\lambda)}{\partial \lambda} = \sum\left(\frac{X_i}{\lambda} - 1\right)\quad \text{and}\quad \frac{\partial^2 LL(X_i|\lambda)}{\partial \lambda^2} = \sum -\frac{x}{\lambda^2}.\]
    Thus the Fisher information is
    \[I(\lambda) = -\EE_{\lambda}\left[\sum \frac{1}{2\lambda^2} - \frac{3x^2}{2\lambda^3}\right] = -\frac{n}{2\lambda^2} + \frac{3n}{2\lambda^3}\cdot \lambda = \frac{n}{\lambda^2},\]
    using the fact that $\EE[x]=\lambda$. Finally, Jeffreys prior is
    \[\pi(\theta)\propto I(\theta)^{1/2}\propto \theta^{-1}.\]
    \item $\Norm(0,\theta)$: For a normal distribution with mean $0$ and variance $\theta$,
    \[f_\theta(x) = \frac{1}{\sqrt{2\pi \theta}}\exp\left(\frac{-x^2}{2\theta}\right).\]
    So, the total log likelihood is 
    \[LL(X_i|\theta) = \sum_{i=1}^n \left(-\log{\sqrt{2\pi}} - \frac{1}{2}\log \theta - \frac{X_i^2}{2\theta}\right).\]
    Differentiating,
    \[\frac{\partial LL}{\partial \theta} = \sum_{i=1}^n -\frac{1}{2\theta} + \frac{X_i^2}{2\theta^2} \quad \text{and}\quad \frac{\partial^2 LL}{\partial \theta^2} = \sum_{i=1}^n \frac{1}{2\theta^2} - \frac{3X_i^2}{2\theta^3}.\]
    Thus, the fisher information is
    \[I(\theta) = -E_{\theta}\left[\sum_{i=1}^n \frac{1}{2\theta^2} - \frac{3X_i^2}{2\theta^3}\right] = -\frac{n}{2\theta^2} + \frac{3n}{2\theta^3}\theta = \frac{n}{\theta^2},\]
    where we have used the fact that $\EE[X^2] = \var[X] = \theta$, since the mean is fixed at zero. Finally, Jeffreys prior is 
    \[\pi(\theta)\propto I(\theta)^{1/2}\propto \theta^{-1}.\]
\end{itemize}