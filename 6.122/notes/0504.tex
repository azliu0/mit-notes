\section{May 4, 2023}

\subsection{Unconstrained Optimization}

Given an objective function $f:\RR^n\rightarrow \RR$, our goal is to compute
\[x^* = \argmin_{x\in \RR^n}f(x).\]
We assume that $f$ is continuous and smooth. One idea is \ac{gradient descent}:
\begin{itemize}
    \item generate a sequence of values $x^0\rightarrow x^1\rightarrow \hdots \rightarrow x^t$, which satisfies $f(x^{t+1})\leq f(x^t)$ for all $t$. 
    \item $x^{t+1} = x^t - \eta \nabla f(x^t)$.
\end{itemize}

\begin{definition}
\deflabel

$f$ is \ac{$\beta$-smooth} for $\beta\geq 0$ if and only if, for all $x,\delta$, 
\[\delta^T\nabla^2f(x)\delta \leq \beta \norm{\delta}^2.\]
\end{definition}

Recall that $\nabla^2f(x)$ is the Hessian matrix, with $\nabla^2f(x)_{i,j} = \partial^2f(x)/(\partial x_i\partial x_j)$. The statement of $\beta$-smoothness is equivalent to saying that the largest eigenvalue of the Hessian has magnitude less than $\beta$. 

We can show that if $f$ is $\beta$-smooth, then 
\[f(x+\delta) \leq f(x) + \nabla f(x)^T\delta + \frac{1}{2}\beta \norm{\delta}^2.\]
Plugging in $\delta = -\eta \nabla f(x)$,
\[f(x+\delta) = f(x) - \eta \norm{\nabla f(x)}^2 + \frac{\beta \eta^2}{2}\norm{\nabla f(x)}^2.\]
Since we want to make progress at each step, 
\[\eta \norm{\nabla f(x)}^2 \geq \frac{\beta\eta^2}{2}\norm{\nabla f(x)}^2\implies \eta \leq \frac{2}{\beta}.\]
If we choose $\eta = 1/\beta$, then we guarantee 
\[f(x^{t+1})-f(x^t) \leq -\frac{1}{2\beta}\norm{\nabla f(x)}^2.\]

\begin{definition}
\deflabel

$f$ is convex if and only if 
\[f(x+\delta) \geq f(x) + \nabla f(x)^T\delta\]
for all $x,\delta$.
\end{definition}

In other words, convexity means that ``the function always lies above the tangent plane''. 

\begin{definition}
\deflabel

$x$ is an \ac{$\varepsilon$-optimal} solution if and only if $f(x)-f(x^*)\leq \varepsilon$. 
\end{definition}

Assuming that $f$ is convex, we can get arbitrarily close to $x^*$. We would like to find how many steps of GD it would take to achieve $x^t$ $\varepsilon$-optimal.

Let $x=x^t$ and $\delta = x^*-x^t$ in the convexity condition. Then, 
\[f(x^*) \geq f(x^t) + \nabla f(x^t)^T(x^*-x^t).\]
This implies by Cauchy-Schwarz:
\[f(x^t)-f(x^*) \leq -\nabla f(x^t)^T(x^*-x^t) \leq \norm{\nabla f(x^t)}\norm{x^*-x^t}\leq \varepsilon.\]
Therefore, the gradient must satisfy 
\[\norm{\nabla f(x^t)} \leq \frac{\varepsilon}{\norm{x^*-x^t}}.\]
\begin{definition}
\deflabel

$f$ is \ac{$\alpha$-strongly convex} for $\alpha>0$ if and only if 
\[\delta^T\nabla f(x)\delta \geq \alpha \norm{\delta}^2,\]
for all $x,\alpha$.
\end{definition}

We can show that if $f$ is $\alpha$-strongly convex, 
\[f(x+\delta)\geq f(x) + \nabla f(x)^T\delta + \frac{\alpha}{2}\norm{\delta}^2.\]
Let $x=x^*$ and $\delta = x^t-x^*$. Then, 
\[f(x^t)\geq f(x^*) + \nabla f(x^*)^T\delta + \frac{\alpha}{2}\norm{\delta}^2 = f(x^*) + 
\frac{\alpha}{2}\norm{\delta}^2\implies f(x^t)-f(x^*)\geq \frac{\alpha}{2}\norm{x^t-x^*}^2.\]
This shows that 
\[\norm{\nabla f(x^t)}^2\norm{x^*-x^t}^2\geq (f(x^t)-f(x^*))^2 \geq \left(\frac{\alpha}{2}\norm{x^t-x^*}^2\right)^2\implies \norm{\nabla f(x^t)}^2 \geq \frac{\alpha (f(x^t) - f(x^*))}{2}.\]
Also, we previously showed $f(x^{t+1})-f(x^{t})\leq -(1/2\beta)\norm{\nabla f(x^t)}^2$, so 
\begin{align*}
    f(x^{t+1})-f(x^*) &\leq f(x^t)-f(x^*)-\frac{1}{2\beta}\frac{\alpha}{2}(f(x^t)-f(x^*))\leq (f(x^t)-f(x^*))\left(1-\frac{1}{4\kappa}\right),
\end{align*}
where $\kappa = \beta/\alpha > 1$ is the \ac{condition number} of $f$. This implies that each step of our GD decays the distance between $x^t$ and the global minimum by a factor of $1/\kappa$, so the number of steps required is 
\[O\left(\kappa \log \frac{f(x^0)-f(x^*)}{\varepsilon}\right).\]

\begin{theorem}
\thmlabel

If $f$ is $\beta$-smooth and $\alpha$-strongly convex, then for any $\varepsilon > 0$, there exists constant $c$ such that $x^t$ is $\varepsilon$-optimal for any 
\[t\geq c\cdot \kappa\log\frac{f(x^*)-f(x^0)}{\varepsilon}.\]
\end{theorem}

Note that $\kappa$ roughly tells us how convex the function is; $\beta$ bounds the Hessian eigenvalues from above, and $\alpha$ bounds them from below. 