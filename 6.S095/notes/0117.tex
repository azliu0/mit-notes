\section{January 17, 2023}

\subsection{Independence}

\begin{definition}
\deflabelname{Independence}

Two events $A,B\in \mathcal{F}$ are independent if 
\[\PP[A\cap B] = \PP[A]\cdot \PP[B].\]

Alternatively, two random variables $X,Y$ defined on the same probability space are said to be independent if, for any $A,B\in \mathcal{B}_{\RR}$,
\[\PP[X\in A, Y\in B] = \PP[X\in A]\cdot \PP[Y\in A].\]
\end{definition}

This definition is the same as the joint density definition that we applied last lecture. 

\begin{theorem}
\thmlabel

For random variables $X,Y$ defined on the same probability space, the following are equivalent: 
\begin{itemize}
    \item $\PP[X\in A, Y\in B] = \PP[X\in A]\PP[Y\in B]$ for every $A,B\in \mathcal{B}_{\RR}$. 
    \item $f_{X,Y}(x,y) = f_X(x)f_Y(y)$ for every $x,y\in \RR$. 
\end{itemize}
\end{theorem}

\begin{proof}
First assume $X\in A$ and $Y\in B$. Then set $A = [x+dx]$ and $B = [y+dy]$ for some small enough $dx,dy$ such that $f_X, f_Y, f_{X,Y}$ are constant over $A$, $B$, and $A\times B$. Then, 
\[\PP[X\in A]\PP[Y\in B] = \left(\int_A f_X(a)\ddd a\right)\left(\int_B f_Y(b)\ddd b\right) = (f_X(x)\ddd x)(f_Y(y)\ddd y).\]

But also, 
\[\PP[X\in A, Y\in B] = \iint_{(a,b)\in A\times B}f_{X,Y}(a,b)\ddd a\ddd b = f_{X,Y}\ddd x\ddd y,\]
which is enough to imply that $f_Xf_Y = f_{X,Y}$. Conversely, assume that $f_Xf_Y = f_{X,Y}$. Writing out the integral for $\PP[X\in A, Y\in B]$, we can separate $f_{X,Y}$ into its two marginal distributions and then integrate separately, which gives $\PP[X\in A, Y\in B] = \PP[X\in A]\PP[Y\in B]$. 
\end{proof}

Now we introduce mutual independence, which is a way to deal with independence between more than just two random variables. 

\begin{definition}
\deflabel

Random variables $X_1, \hdots, X_n$ are \ac{mutually independent} if for any sets $A_1, \hdots, A_n\in \mathcal{B}_{\RR}$, 
\[\PP\left[\bigcap_i(X_i\in A_i)\right] = \prod_{i}\PP[X_i\in A_i].\]
\end{definition}

\begin{example}
\exlabel

Mutual independence implies pairwise independence.
\end{example}

\begin{proof}
WLOG, we show that $X_1$ and $X_2$ are pairwise independent. For all $i\geq 3$, let $A_i = \RR$. Since $X_i\in A_i$ is always true for $i\geq 3$, this implies 
\[\PP\left[\bigcap_i(X_i\in A_i)\right] = \PP[X_1\in A_1, X_2\in A_2].\]
Also, $\PP[X_i\in A_i]=1$ for all $i\geq 3$, so the product on the right is the same as $\PP[X_1\in A_1]\PP[X_2\in A_2]$. Thus, $X_1$ and $X_2$ are pairwise independent.
\end{proof}

\begin{example}
\exlabel

Pairwise independence does not imply mutual independence. 
\end{example}

Let $X_1, X_2, X_3\in \{0,1\}$ be the results of three independent flips of a fair coin. Let $Y_1=0$ when $X_2=X_3$ and $Y_1=1$ otherwise. Define $Y_2$, $Y_3$ analogously. 

$Y_i$ are not mutually independent, since $\PP[Y_1=Y_2=Y_3=1] = 0$, while $\PP[Y_1=1]\PP[Y_2=1]\PP[Y_3=1] = 1/8$. 

On the other hand, $Y_i$ are pairwise independent, since each pair of $Y_i$ occurs with probability $1/4 = 1/2\cdot 1/2$, which is also the product of their marginal distributions. 

\subsection{Conditional Probability}

\begin{definition}
\deflabel

Given two events $A,B$, the conditional probability of $A$ given $B$ is defined as
\[\PP[A | B] = \frac{\PP[A\cap B]}{\PP[B]}.\]

Conditional probabilities when $A$ and $B$ are random variables is defined analogously. 
\end{definition}

Note that $\PP[A|B] = \PP[A]$ when $A$ and $B$ are independent. 

\begin{theorem}
\lemlabel

Conditional probabilities are probability measures. 
\end{theorem}

\begin{proof}
Conditional probability satisfies the two laws governing probability measures:
\begin{itemize}
    \item $0\leq \PP[A|B]\leq 1$. 
    \item For disjoint events $A_i$, 
    \[\PP\left[\left(\bigcup_i A_i\right)\vert B\right] = \frac{\PP[(\cup_i A_i)\cap B]}{\PP[B]} = \frac{\sum_i \PP[A_i\cap B]}{\PP[B]} = \sum_i \PP[A_i\vert B]. \]
\end{itemize}
\end{proof}

\begin{theorem}
\thmlabelname{Law of total probability}

Let $Y$ be a random variable taking discrete values $y_1, \hdots, y_n$. For any event $A$, 
\[P[X\in A] = \sum_{i=1}^n\PP[Y=y_i]\PP[X\in A|Y=y_i].\]
\end{theorem}

\begin{proof}
The events $\{Y=y_i\}$ partition the sample space, so 
\[\sum_{i=1}^n\PP[(X\in A)\cap (Y=y_i)] = \PP[X\in A|(\cap_{i=1}^n\{Y=a_i\})] = \PP[X\in A].\]
\end{proof}

\begin{theorem}
\thmlabelname{Bayes' rule}

For random variables $X,Y$, 
\[\PP[X\vert Y] = \frac{\PP[Y\vert X]\PP[X]}{\PP[Y]}.\]
\end{theorem}

\subsection{Inference}

Bayes' rule is important to the field of \ac{inference}, which is important for probability statistics, information theory, machine learning, etc. Inference involves inferring properties of some random variable $(Y)$ via data $(X)$. 

\begin{example}
\exlabel

Most people have never had the same Uber driver twice. How can we use this observation to estimate the number of Uber drivers in Boston?
\end{example}

In this case, 
\begin{itemize}
    \item $X$ is the observation that I have never had the same uber driver twice.
    \item $Y$ is the number of Uber drivers in Boston.
\end{itemize}

Let's try guessing values for $Y$ and seeing what happens. 

\begin{definition}
\deflabel

Given data $X=x$ and random variable $Y$ to be estimated, the \ac{maximum likelihood estimation} for the value of $Y$ is 
\[\yhat_{MLE} = \underset{y}{\text{arg max }}p_{X\vert Y}(x\vert y).\]
\end{definition}

Intuitively, we're calculating the probability that we observe the data $x$ (i.e., I have never had the same Uber driver twice), over all possible values for the random variable $Y$ (i.e., the number of Uber drivers in Boston). Then, the value of $Y$ that gives us the greatest probability is our MLE. 

If we're equally likely to get any driver on every ride, the probability of our observation assuming $Y=y$ is 
\[p_{X\vert Y}(x\vert y) = \frac{y(y-1)\cdots (y-n+1)}{y^n}.\]

This function is increasing in $y$, so $\yhat_{MLE}=\infty$, which is not useful. To fix this, we can further impose a \ac{prior} on $Y$. 

\begin{definition}
\deflabel

Given data $X=x$ and random variable $Y$ to be estimated, the \ac{maximum a posteriori} (MAP) estimation for the value of $Y$ is 
\[\yhat_{MAP} = \underset{y}{\text{arg max }}\frac{p_{X\vert Y}(x\vert y)p_Y(y)}{p_X(x)} = \underset{y}{\text{arg max }}p_{X\vert Y}(x\vert y)p_Y(y).\]
\end{definition}

The prior is represented by the distribution $p_{Y}(y)$, and represents some prior belief that we have about the distribution of $Y$. For example, we know that $Y$ must be less than the total number of people in Boston, which is something that can be captured by this prior distribution. The way our modified formula works is by Bayes' rule; we wish to maximize the probability of $Y$ given $X$ over all values of $y$, which is the quantity inside of the arg max. 

One reasonable prior we might use for this scenario is the \ac{log-normal distribution}, which has pdf
\[p_Y(y) = \frac{1}{y\sigma \sqrt{2\pi}}\text{exp }\left(-\frac{(\ln{y} - \mu)^2}{2\sigma^2}\right).\]

Another example to consider:

\begin{example}
\exlabel

Let $X_1, \hdots, X_n$ drawn independently from $\mathcal{N}(\mu, 1)$. Also, suppose that we know that $\mu$ should be concentrated around zero, so we can impose the modeling assumption $\mu\sim \mathcal{N}(0,1)$. What is the posterior distribution $p_{\mu\vert X_1, \hdots, X_n}$?
\end{example}

Let $x_1, \hdots, x_n$, and $\mu_0$, denote the data that we observe. Each $X_i$ is drawn independently, so we have: 
\begin{align*}
    p_{X_1, \hdots, X_n\vert \mu}(x_1, \hdots, x_n\vert \mu_0) &= \prod_{i=1}^n p_{X_i\vert \mu}(x_i\vert \mu_0) \\ 
    &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}e^{-(x_i-\mu_0)^2/2}.
\end{align*}

Our prior distribution is given by $p_{\mu}(\mu_0)\sim \mathcal{N}(0,1)$. Also, the posterior function is a function of $\mu_0$ and $x_1, \hdots, x_n$ only. Since each $X_i$ represents our data, we can treat them like constant values with respect to our posterior distribution, and therefore ignore terms like $p_{X_1, \hdots, X_n}(x_1, \hdots, x_n)$ (this value is constant for fixed data). Applying Bayes' rule, we now have  
\begin{align*}
    p_{\mu | X_1, \hdots, X_n}(\mu_0 | x_1, \hdots, x_n) \propto p_{X_1, \hdots X_n | \mu}(x_1, \hdots, x_n | \mu_0)p_{\mu}(\mu_0) \\
    \propto \exp \left(-\frac{1}{2}\sum_{i=1}^n(x_i-\mu_0)^2 - \frac{1}{2}\mu_0^2\right) \\
    \propto \exp \left(\frac{n+1}{2}\left(\mu_0 - \frac{\sum_{i=1}^nx_i}{n+1}\right)^2\right),
\end{align*}
which comes from expanding and completing the square with respect to $\mu_0$ (and absorbing terms into the proportionality). This implies 
\[p_{\mu | X_1, \hdots, X_n}(\mu_0 | x_1, \hdots, x_n) \propto \mathcal{N}\left(\frac{n}{n+1}\overline{x}, \frac{1}{n+1}\right).\]

Intuitively, this makes sense. With no prior, we expect the distribution of $\mu$ to be centered around $\overline{x}$, since this is the only data we are given. Given the prior, i.e., the expectation that $\mu$ is actually distributed normally around $0$, the center of the posterior distribution is pulled closer to zero. 

\subsection{Discrete Conditioning}

\begin{definition}
\deflabel

Two events $A,B$ are conditionally independent given $C$ if 
\[\PP[A\cap B | C] = \PP[A | C]\cdot \PP[B | C].\]

Analogously, let $X,Y,Z$ be random variables. $X$ and $Y$ are conditionally independent given $Z$ if for any $A,B,C\in \mathcal{B}_{\RR}$, 
\[\PP[X\in A, Y\in B | Z\in C] = \PP[X\in A | Z\in C] \cdot \PP[Y\in B | Z\in C].\]
\end{definition}

\begin{example}
\exlabel

Consider the same coin flipping example as previously.
\end{example}

$Y_1,Y_2$ are conditionally independent given $X_3$, since knowing the result of $X_3$ provides no information in deducing $Y_1$ or $Y_2$. However, $Y_1$ and $Y_2$ are not conditionally independent given $Y_3$, since given $Y_3=1$, it is not possible for $Y_1=1$ and $Y_2=1$. 

\begin{example}
\exlabel

Conditional independence does not imply marginal independence. 
\end{example}

Consider two coins, one fair and the other with two heads. Let $Z$ denote a random choice of either coin. Then, let $X$, $Y$ denote two flips of this coin. Given $Z$, $X$ and $Y$ are independent, so $X$ and $Y$ are conditionally independent on $Z$. On the other hand, they are not marginally independent; given that $X$ is heads, the probability that the coin with two heads was chosen is $2/3$, and therefore the probability that $Y$ is also heads increases. 

\begin{example}
\exlabel

Marginal independence does not imply conditional independence.
\end{example}

Let $X$ and $Y$ be the outcomes of two flips of a fair coin. If $Y=H$, Let $Z=X$; otherwise, let $Z=!X$. $X$ and $Z$ are marginally independent with no information about $Y$. On the other hand, given $Y$, $X$ and $Z$ are no longer independent, so $X$ and $Z$ are not conditionally independent.

\begin{definition}
\deflabel

Given discrete random variables $X,Y$, the \ac{conditional expectation} of $X$ given $Y$ is 
\[\EE[X\vert Y] = \sum_{x\in X(\Omega)}x\PP[X=x | Y].\]

The analogous statement for continuous random variables is 
\[\EE[Y | X=x] = \int_{\RR} yf_{Y|X}(y,x)\ddd y,\]
where $f_{Y|X}(y,x)$ is the \ac{conditional density} of $Y$ given $X=x$. 
\end{definition}

The conditional density satisfies 
\[f_{X,Y}(x,y) = f_{Y|X}(y,x)f_X(x) = f_{X|Y}(x,y)f_Y(y),\]
where $f_{X,Y}(x,y)$ is the joint density of $X$ and $Y$. Also note that $\EE[X\vert Y]$ is a function of $Y$, so this quantity is itself also a random variable.

\begin{definition}
\deflabel

The \ac{conditional variance} of $X$ given $Y$ is 
\[\var[X | Y] = \EE[(X - \EE[X\vert Y])^2\vert Y].\]
\end{definition}

\begin{theorem}
\lemlabelname{Self-conditioning}

Given random variable $X$ and deterministic function $f$, then
\[\EE[f(X)\vert X] = f(X).\]
\end{theorem}

\begin{proof}
Assume $X=y$. Then
\[\EE[f(X)|X=y] = \sum_{x\in f(X)(\Omega)}x\PP[f(X)=x\vert X=y].\]
All probabilities are zero unless $x=f(y)$, in which case the probability is $1$, so 
\[\EE[f(X)|X=y] = f(y).\]
Since this is true for any possible value of $X$, $\EE[f(X)|X] = f(X)$, as desired. 
\end{proof}


\begin{theorem}
\proplabelname{Conditional Linearity of Expectation}

For any real constants $c_1, c_2$ and random variables $X_1, X_2, Y$, linearity of conditional expectation holds. That is, 
\[\EE[c_1X_1 + c_2X_2 | Y] = c_1\EE[X_1|Y] + c_2\EE[X_2|Y].\]
\end{theorem}

\begin{example}
\exlabel

Let $X,Y$ be the results of two independent rolls of a fair $6$-sided die, and let $Z = X+Y$. Compute $\EE[X|Z]$ and $\var[X|Z]$. 
\end{example}

By symmetry, $\EE[X|Z] = Z/2$. Now the variance: 
\[\var[X|Z] = \EE[(X-\EE[X|Z])^2|Z] = \EE[(X-Z/2)^2|Z].\]

OK, now this is ugly. Given $Z$, $X$ ranges on an interval from $\max{(1, Z-6)}$ to $\min{(6, Z-1)}$, inclusive. Say that this interval has start $a$ and end $a+\ell-1$. Then $Z = 2a+\ell-1$, and summing $(X-Z/2)^2$ over this interval gives us 
\[\sum_x (x-Z/2)^2 = \sum_{i=1}^{\ell}\left(\frac{-\ell + (2i-1)}{2}\right)^2 = 2\sum_{i=1}^{\ell-(2i-1) > 0}\left(\frac{l-(2i-1)}{2}\right)^2.\]

If $\ell$ is odd, then we're just summing the first $((\ell-1)/2)$ squares, in which case
\[\sum_x (x-Z/2)^2 = 2\frac{(\ell-1)/2\cdot \ell\cdot (\ell+1)/2}{6} = \frac{1}{12}\ell(\ell^2-1).\]

If $\ell$ is even, then
\[2\sum_{i=1}^{\ell-(2i-1) > 0}\left(\frac{l-(2i-1)}{2}\right)^2 = \frac{1}{2}\sum_{i=1}^{\ell-(2i-1) > 0}(\ell-(2i-1))^2,\]
which is the sum of the odd squares from $1$ to $(\ell-1)$, which is also the sum of the first $(\ell-1)$ squares, minus the sum of the even squares from $2$ to $(\ell-2)$. Therefore, 
\[\sum_x (x-Z/2)^2 = \frac{1}{2}\left(\frac{\ell(\ell-1)(2\ell-1)}{6} - 4\cdot \frac{(\ell-2)/2\cdot (\ell/2)\cdot (\ell-1)}{6}\right) = \frac{1}{12}\ell(\ell^2-1).\]

The sum turns out to be the same in both cases. Since the length of the interval is $\ell$, the expected value is $(\ell^2-1)/12$. Therefore, 
\[\var[X \vert Z] = \EE[X-Z/2 \vert Z] = (\left(\min{(6, Z-1)} - \max{(1, Z-6)} + 1\right)^2 - 1)/12.\]

\subsection{Continuous Conditioning}

Polar coordinates are a thing. Let $(X,Y)$ be drawn from a probability distribution on the plane, and define $(R, \Theta)$ so that 
\[X = R\cos{\Theta}, Y = R\sin{\theta}, 0\leq R, 0\leq \Theta < 2\pi.\]
By the Jacobian,
\[p_{R, \Theta}(r, \theta) = rp_{X,Y}(x,y).\]

\begin{example}
\exlabel

Let $(X,Y)$ be a randomly chosen point in the interior of the unit disc. Compute $\EE[X^2+Y^2]$. 
\end{example}

We know $p_{X,Y}(x,y) = 1/\pi$ is uniform, so $p_{R,\Theta}(r,\theta) = rp_{X,Y}(r\cos{\theta}, r\sin{\theta}) = r/\pi$. Notice that this factors: 
\[p_R(r) = 2r\quad \text{and} \quad p_{\Theta}(\theta) = \frac{1}{2\pi},\]
hence $R$ and $\Theta$ are independent. Now, our expectation is
\[\EE[X^2+Y^2] = \EE[R^2] = \int_0^1 r^2(2r\ddd r) = \frac{1}{2}.\]

\begin{example}
\exlabel

In the notation of the previous example, evaluate $\EE[X^2+Y^2\vert X]$. 
\end{example}

By the self-conditioning Lemma, $\EE[X^2|X] = X^2$, so it suffices to compute $\EE[Y^2|X]$.
This can be computed by first computing the density $f_{Y|X}(y,x) = f_{X,Y}(x,y)\cdot f_{X}(x)$, and then integrating
\[\EE[Y^2|X] = \int y^2f_{Y|X}(y,x)\ddd y.\]

\begin{example}
\exlabel

Compute the Beta integral
\[I_{a,b} = \int_0^1 x^a(1-x)^b\ddd x.\]
\end{example}

Let $X_1, \hdots, X_{a+b+1}$ be independent and identically distributed random variables in $[0,1]$, all uniform. Let $E$ be the event that $X_1$ is the $(a+1)$th smallest among the $X_i$. Then, 
\[\PP[E | X_1=x] = \binom{a+b}{a}x^a(1-x)^b,\]
since $\PP[X_i\leq x]=x$ for uniform variables, and we need to choose $a$ to be less than $X_1$. Also, the pdf of $X_1$ is constant, i.e., $p_{X_1}(x) = 1$, since $X_1$ is uniformly distributed. Therefore, by the law of total probability, 
\[\PP[E] = \int_0^1\PP[E \vert X_1=x]p_{X_1}(x)\ddd x = \binom{a+b}{a}I_{a,b}.\]

On the other hand, by symmetry, $\PP[E] = 1/(a+b+1)$. Therefore, 
\[I_{a,b} = \frac{1}{(a+b+1)\binom{a+b}{a}} = \frac{a!b!}{(a+b+1)!}.\]



