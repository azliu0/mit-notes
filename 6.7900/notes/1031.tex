\section{October 31, 2023}

\subsection{Variational Autoencoders}

Another generative model is the variational autoencoder. The high level goal of VAEs is to compress a complex data distribution into a smaller latent space, and then be able to regenerate the distribution from latent space. 

In particular, the \ac{inference model}, or \ac{encoder}, or \ac{recognition model} attempts to learn $q_{\phi}(z|x)\approx P_{\theta}(z|x)$, where $z$ is some data in the latent space, and $x\sim P_{\theta}$ is the true data distribution. 

This encoding process is very similar to the encoding we use for GMMs. In particular, we can learn $q_{\phi}$ by minimizing log likelihood $\log P_{\theta}(x)$, which we can do with the ELBO lower bound (we quickly derive another way to write this expression):
\begin{align*}
	&\mathcal{L}_{\theta, \phi}(x) = \sum_z q_{\phi}(z|x)\log(P_{\theta}(x)P_{\theta}(x|z)) + \sum_z q_{\phi}(z|x)\log\frac{1}{q_{\phi}(z|x)} \\
	&\quad= \sum_z q_{\phi}(z|x)\log\left(\frac{P_{\theta}(z)P(x|z)}{q_{\phi}(z|x)}\right) = \EE_{q_{\phi}(z|x)}\left(\log\frac{P_{\theta}(x,z)}{q_{\phi}(z|x)}\right). 
\end{align*}

We showed previously that $\log P_{\theta}(x) = \mathcal{L}_{\theta,\phi}(x) + KL(q_{\phi}(z|x), P_{\theta}(z|x))$, where $KL(q_{\phi}(z|x),P_{\theta}(z|x))\geq 0$ with equality iff our model correctly predicts $q_{\phi} = P_{\theta}$. In other words,  
\begin{align*}
	\log P_{\theta}(x) &\geq \int q_{\phi}(z|x)\log \left(\frac{P_{\theta}(x,z)}{q_{\phi}(z|x)}\right) \ddd z + H(q_{\phi}(z|x))
\end{align*}
with equality iff we predict correctly, so we can try optimizing the ELBO lower bound instead of the log likelihood directly.

It is common to reparamaterize the latent space $z\sim q_{\phi}(z|x) = \Norm(z; \mu(x,\phi),\text{\diag}(\sigma(x;\phi)^2))$, where $\mu(x;\phi)$ and $\sigma(x;\phi)$ completely specifies the posterior distribution. These can be learned with a neural net, e.g., 

\begin{center}
	$(\mu, \log \sigma) = \text{EncoderNeuralNet}_{\phi}(x)$ \\
	$q_{\phi}(z|x) = \Norm(z; \mu, \text{\diag}(\sigma))$.
\end{center}

During learning, it is common to represent $q_{\phi}$ as a function of some externally chosen noise; for example, we might sample $\varepsilon\sim \Norm(0,1)$, and then take $z_{\phi} = \mu(x;\phi) + \sigma(x;\phi)\odot \varepsilon$. This way, we can backprop from the final prediction through the latent space and back to $\phi$. If we sample directly from the latent space, we can't backprop (we can compute the gradient when $\varepsilon$ is an input to the function, but we can't when we have to sample $\varepsilon$ as part of the function). 

The \ac{generative model}, or \ac{decoder}, remaps data from the latent space into the complex data space. This part of the model attempts to learn $P_{\theta}(x|z)$, which is also commonly reparamaterized as a collection of gaussians $\Norm(z;g_{\theta}(z), \sigma^2I)$. Parameters $g,\sigma$ can again be learned with a neural net, $\text{DecoderNeuralNet}_{\theta}(z)$. Lastly, training can be done with EM, similar to the EM from Gaussian mixtures: 
\begin{itemize}
	\item decoder: 
		\[\theta = \theta + \eta \nabla_{\theta}\log P_{\theta}(x|z_{\phi}).\]
	\item encoder: 
		\[\phi = \phi + \eta \nabla_{\phi}\left(\log P_{\theta}(x|z_{\phi}) - KL(q_{\phi}(z|x) | P_{\theta}(z))\right).\] 
\end{itemize}
where gradient ascent updates are made according to the ELBO criterion.


