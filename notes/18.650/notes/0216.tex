\section{February 16, 2023}

\subsection{Maximum Likelihood Estimation}

We are given i.i.d. $x_1, \hdots, x_n$ data from a statistical model $(\Omega, (\PP_{\theta})_{\theta\in \Theta})$. Suppose $\Omega$ is a discrete probability space. Our goal is to find an estimator $\hat{\theta}$ that maximizes our \ac{likelihood function} 
\[L(x_1, \hdots, x_n; \theta) = \prod_i\PP_{\theta}(X_i = x_i).\]

This is the same as maximizing the \ac{Log-Likelihood}:
\[\log L(x_1, \hdots, x_n; \theta) = \sum_{i=1}^n\log \PP_{\theta}(X_i=x_i).\]

Our \ac{maximum likelihood estimator} is given by 
\[\hat{\theta} = \argmax_{\theta}(\log L(x_1, \hdots, x_n; \theta)).\]

In the case where $\Omega$ is continuous, all definitions are the same, except we replace $\PP_{\theta}$ with $f_{\theta}$, the density of $\PP_{\theta}$. 


\begin{example}
\exlabel\hypertarget{ex:mlebern}{}

Compute the maximum likelihood estimator (MLE) given $n$ data points from the statistical model $(\{0,1\}, (\Bern(p))_{p\in (0,1)})$.
\end{example}

Note that $\PP_{\theta}(X_i=x_i) = p^{x_i}(1-p)^{1-x_i}$. Therefore, 
\[\log L(\theta) = n(\ov{X}_n\log \theta + (1-\ov{X}_n)\log (1-\theta)),\]
and 
\[\frac{\partial \log L}{\partial \theta} = n\left(\frac{\ov{X}_n}{\theta} - \frac{1-\ov{X}_n}{1-\theta}\right).\]
Setting the derivative to zero, we get $\hat{\theta} = \hat{p} = \ov{X}_n$ as our MLE. This agrees with our intuition. 

\begin{example}
\exlabel

Consider the statistical model $(\RR, (\Norm(\mu, \sigma^2))_{(\mu, \sigma)\in \RR\times (0, \infty)})$. 
\end{example}

From the density of the normal distribution, we have
\[L(\mu, \sigma^2) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n\exp \left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right),\]
and 
\[\log L(\mu, \sigma^2) = -\frac{n}{2}\log (2\pi \sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2.\]
Maximizing:
\[\frac{\partial \log L}{\partial \sigma^2} = \frac{-n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^{n}(x_i-\ov{X}_n)^2.\]
So, our best variance estimator is given by 
\[\hat{\sigma^2} = \frac{1}{n}\sum_{i=1}^n(x_i - \ov{X}_i)^2.\]

\subsection{Relative Entropy}

The motivation behind the way that we calculate our maximum likelihood estimator is to get our estimated distribution as close as possible to the actual distribution. If we let $\theta^*$ be the true parameter, then 
\begin{align*}
    \hat{\theta} &= \argmax_{\theta}\left(\frac{1}{n}\sum_{i=1}^n\log f_{\theta}(X_i=x_i)\right) \\
&= \argmin_{\theta}\left(\frac{1}{n}\sum_{i=1}^n\log f_{\theta^*}(X_i=x_i)-\frac{1}{n}\sum_{i=1}^n\log f_{\theta}(X_i=x_i)\right).
\end{align*}

By the strong law of large numbers, the quantity inside of the argmin converges almost surely to 
\[\EE_{\theta^*}\left[\log \frac{f_{\theta^*}(X)}{f_{\theta}(X)}\right].\]

\begin{definition}
\deflabel

The \ac{relative entropy} between the actual distribution $\PP_{\theta^*}$ and some other $\PP_{\theta}$ is given by 
\[\EE_{\theta^*}\left[\log \frac{f_{\theta^*}(X)}{f_{\theta}(X)}\right].\]
\end{definition}

This quantity is also called \ac{Kullback-Leibler (KL) divergence}. This shows that computing the MLE is the same as minimizing the relative entropy between the actual distribution, and our predicted distribution. 

\subsection{Fisher Information, Cramer Rao}

Let $\ell(x, \theta) = \log L(x, \theta)$, and suppose $\Theta\subseteq \RR$. 
\begin{definition}
\deflabel

The \ac{Fisher Information} $I(\theta)$ is given by 
\[I(\theta) = \var_{\theta}\left(\frac{\partial}{\partial \theta}\ell(x, \theta)\right) = -\EE_{\theta}\left(\frac{\partial^2}{\partial^2\theta}\ell(x, \theta)\right).\]
\end{definition}

Note that we are taking the expectation and variance of the inner functions with respect to $\theta$ (i.e., averaging over all possible $x$). It's not immediately clear that these two definitions (expectation and variance) are the same, so we show the proof below:

\begin{proof}
First, we have 
\[\frac{\partial}{\partial \theta}\ell(x,\theta) = \frac{\partial}{\partial \theta}\log f_{\theta}(x) = \frac{\partial/(\partial \theta)(f_{\theta}(x))}{f_{\theta}(x)},\]
and 
\[\frac{\partial^2}{\partial^2 \theta}\ell(x,\theta) = \frac{\left[\partial^2/(\partial^2\theta)(f_{\theta}(x))\right]f_{\theta}(x) - \left[\partial/(\partial\theta)(f_{\theta}(x))\right]^2}{f_{\theta}(x)^2}.\]
Now,
\begin{align*}
\EE\left[\frac{\partial}{\partial \theta}\ell(x,\theta)\right]&=\int \left(\frac{\partial/(\partial \theta)(f_{\theta}(x))}{f_{\theta}(x)}\right)\cdot f_{\theta}(x)\ddd x \\
&= \frac{\ddd}{\ddd \theta}\int f_{\theta}(x) \ddd x = 0,
\end{align*}
since $\int f_{\theta}(x)\ddd x = 1$ always. Thus,
\begin{align*}
    \var\left[\frac{\partial}{\partial \theta}\ell(x,\theta)\right] &= \EE\left[\left(\frac{\partial}{\partial \theta}\ell(x,\theta)\right)^2\right] - \EE\left[\frac{\partial}{\partial \theta}\ell(x,\theta)\right]^2 \\
    &= \int \left(\frac{\partial/(\partial \theta)(f_{\theta}(x))}{f_{\theta}(x)}\right)^2f_{\theta}(x)\ddd x \\
    &= \int \frac{(\partial/(\partial \theta)(f_{\theta}(x)))^2}{f_{\theta}(x)} \ddd x.
\end{align*}
On the other hand, 
\begin{align*}
    -\EE\left[\frac{\partial^2}{\partial^2 \theta}\ell(x,\theta)\right] &= -\int \left(\frac{\left[\partial^2/(\partial^2\theta)(f_{\theta}(x))\right]f_{\theta}(x) - \left[\partial/(\partial\theta)(f_{\theta}(x))\right]^2}{f_{\theta}(x)^2}\right)f_{\theta}(x) \ddd x\\
    &= \int \frac{(\partial/(\partial \theta)(f_{\theta}(x)))^2}{f_{\theta}(x)} \ddd x - \frac{\partial}{\partial \theta}\EE\left[\frac{\partial}{\partial \theta}\ell(x,\theta)\right] \\
    &= \var\left[\frac{\partial}{\partial \theta}\ell(x,\theta)\right],
\end{align*}
as desired.
\end{proof}

Recall that the bias of an estimator $\hat{\theta}$ of $\theta$ is given by 
\[\text{bias}(\hat{\theta}) = \EE[\hat{\theta}] - \theta.\]
An unbiased estimator is any estimator with no bias.

\begin{definition}
\deflabel

The \ac{Cramer-Rao} lower bound gives a lower bound to the variance of any unbiased estimator. In particular, given unbiased estimator $\hat{\theta}$ for $\theta$, 
\[\var(\hat{\theta})\geq \frac{1}{I(\theta)}.\]
\end{definition}

\begin{theorem}
\thmlabelname{The MLE is consistent and normal}

Let $\theta^*\in \Theta$ be the true parameter, with some technical conditions holding (e.g., the support of $f_{\theta}$ cannot depend on $\theta$). Then, 
\begin{itemize}
    \item $\hat{\theta}_n^{MLE}$ is a consistent estimator, i.e., 
    \[\hat{\theta}_n^{MLE}\cp \theta^*.\]
    \item It is asymptotically normal: 
    \[\sqrt{n}(\hat{\theta}_n^{MLE} - \theta^*)\cd \Norm(0, 1/I(\theta^*)).\]
\end{itemize}
\end{theorem}

In particular, by Cramer-Rao, this implies that the MLE gives us the best possible variance. This theorem demonstrates that, in theory, the MLE gives us everything we want in an estimator. In reality, it is often difficult to compute the MLE, so we have to resort to more practical estimators that aren't as perfect. 

\begin{example}
\exlabel

As usual, let's return to the kissing experiment.
\end{example}

In this case, our statistical model was
\[(\{0,1\}, (\Bern(p))_{p\in (0,1)}).\]
We showed \hyperlink{ex:mlebern}{here} that the MLE for this model is $\hat{\theta}_n^{MLE} = \ov{X}_n$. Moreover, 
\[\ell(x,\theta) = x\ln \theta + (1-x)\ln (1-p),\]
and
\[\frac{\partial}{\partial \theta}\ell(x,\theta) = \frac{x}{\theta} - \frac{1-x}{1-\theta},\]
and 
\[\frac{\partial^2}{\partial^2\theta}\ell(x,\theta) = -\frac{x}{\theta^2} - \frac{1-x}{(1-\theta)^2},\]
so
\[I(\theta) = \frac{1}{\theta}+\frac{1}{1-\theta} = \frac{1}{\theta(1-\theta)}.\]
Using our theorem, we can put everything together:
\[\sqrt{n}(\hat{p}-p)\cd \Norm(0,p(1-p)),\]
which is what we wanted. 
