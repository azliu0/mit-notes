\section{April 4, 2023}

\subsection{Principal Component Analysis}

The setup for PCA is that we have some high dimensional data that can be clustered approximately into clouds. We want to project the data into a lower dimension such that the variation between the clouds is still captured. Lower dimensional data saves computational power; we want to sacrifice as little of the depth of the data as possible by doing reducing its dimensionality. 

\hrulebar

$X_1, \hdots, X_n\in \RR^d$. $\XX=[X_1^T, \hdots, X_n^T]^T\in \RR^{n\times d}$. The matrix
\[S = \frac{1}{n}\sum_{i=1}^n(X_i-\ov{X})(X_i-\ov{X})^T\]
is the sample covariance matrix. 

-- unclear: the professor says that the usage of $1/n$ instead of $1/(n-1)$ for sample covariance / variance is specific to PCA --

Let $U\in \RR^d$. We want $U^TX_1, U^TX_2, \hdots, U^TX_n = Y_1, \hdots, Y_n$ to have maximal variation. The sample variance of $Y_1, \hdots, Y_n$ is 
\[\frac{1}{n}\sum_{i=1}^n(Y_i-\ov{Y})^2.\]

\begin{theorem}
\claimlabel

The sample variance 
\[\frac{1}{n}\sum_{i=1}^n (Y_i-\ov{Y})^2 = U^TSU.\]
\end{theorem}

\begin{proof}
We have $Y_i-\ov{Y} = U^TX_i-U^T\ov{X}_n = U^T(X_i-\ov{X}_n)$. Therefore, \[(Y_i-Y)^2 = (Y_i-\ov{Y})\cdot (Y_i-\ov{Y}) = U^T(X_i-\ov{X}_n)(X_i-\ov{X}_n)^TU,\]
since $(Y_i-\ov{Y})$ is a scalar and equal to its own transpose. Thus, 
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n(Y_i-\ov{Y})^2 = U^T\left(\frac{1}{n}\sum (X_i-\ov{X}_n)(X_i-\ov{X}_n)^T\right)U = U^TSU. 
\end{align*}
\end{proof}

\begin{definition}
\deflabelname{Positive Definite Matrices}

If $A$ is a symmetric matrix, $A$ is positive definite if all eigenvalues of $A$ are positive. $A$ is positive semidefinite if all eigenvalues of $A$ are nonnegative. Alternatively, $A$ is positive definite if $U^TAU > 0$ for all $U\neq 0$, and $A$ is positive semidefinite if $U^TAU\geq 0$ for all $U\neq 0$. 
\end{definition}

\begin{theorem}
\claimlabel

$S$ is positive semidefinite.
\end{theorem}

\begin{proof}
$U^T(X_i-\ov{X}_n)(X_i-\ov{X}_n)^TU = (U^T(X_i-\ov{X}_n))^2\geq 0$. 
\end{proof}

Fix $U_0$ and define $U_r=rU_0$. Then, $U_r^TSU_r = r^2U_0^TSU_0$. 
\begin{itemize}
    \item For any $U_0$, $U_0^TSU_0\geq 0$. If $U_0^TSU\neq 0$, then we can increase $r$ to make the variance grow unbounded. 
\end{itemize}

The first principle component is $u_1 = \argmax_{\vert U\vert \leq 1}U^TSU$. The second principle component is $u_2 = \argmax_{\vert U\vert\leq 1, U\perp u_1} U^TSU$. We can continue defining the $n$th principle component following this general pattern. These vectors represent the vectors among which the variation in the data is maximized, when the data is projected along these vectors. 

\begin{theorem}
\thmlabel

By the spectral decomposition theorem, any real symmetric matrix $A$ has spectral decomposition 
\[A = PDP^T,\]
where $P$ is an orthonormal matrix containing the eigenvectors of $A$, and $D$ is a diagonal matrix containing the corresponding eigenvalues. 
\end{theorem}

Let $\lambda_1\geq \lambda_2\geq \hdots \geq \lambda_d$ be the eigenvalues of $A$. Using the spectral decomposition of $A$, it is possible to show that the first principle component is given by the eigenvector corresponding to $\lambda_1$. Similarly, the second principle component is the second largest eigenvector subject to the constraint that it is orthogonal to the first eigenspace. And so on.  (See 701 notes for more detail)