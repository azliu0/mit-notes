\section{March 21, 2023}

\subsection{Nonparametric Regression}

Recap from last time: non-parametric regression is when we do not make any parametric assumptions on $f$. For example, a parametric assumption on $f$ would be to assume that $f(x)\in \{a+bx, a+bx+cx^2, e^{a+bx},...\}$. 

One non-parametric regressor is to take \textbf{local averages}. This assumes that $f$ is ``smooth'', and can thus be well approximated by some piecewise constant function, e.g., $f(t)\approx f(x)$ for $t$ close enough to $x$. 

Let $h>0$ be the \ac{window size} or \ac{bandwidth}. Then define 
\[I_x = \{i=1,2,\hdots, n : \vert X_i-x\vert < h\}.\]
Our regressor is then given by 
\[\hat{f}_{n,h}(x) = \begin{cases}
\displaystyle\frac{1}{\vert I_x\vert}\sum_{i\in I_x}Y_i & I_x\neq \emptyset\\
0 & \text{else}.
\end{cases}\]
For this to be a good regressor, we need to choose an appropriate bandwidth. As $h\rightarrow 0$, the model becomes overfit, and as $h\rightarrow \infty$, the model becomes underfit (just a straight horizontal line). 

\begin{example}
\exlabel

We will show one way to choose a ``smart'' value for $h$. Let $x_i=i/n$ for $i\in \{0\}\cup [n]$. Let $Y_i = f(x_i) + \varepsilon_i$ for standard normally distributed $\varepsilon_i$. Suppose that we know $\vert f'(x)\vert\leq L$ for $x\in [0,1]$. 
\end{example}

Construct the non-parametric estimator $\hat{f}(x_i) = (\sum_{j\in I_i}Y_j)/\vert I_i\vert$, where $I_i=\{j : \vert j-i\vert \leq k\}$. 

\begin{theorem}
\claimlabelname{Variance}

\[\var[\hat{f}(x_i)]\leq \frac{1}{k}.\]
\end{theorem}

\begin{proof}
$\var[Y_i]=\var[\varepsilon_i]=1$. Therefore, $\var[\hat{f}(x_i)] = \sum_{j\in I_i}\var[Y_j]/\vert I_i\vert^2 \leq 1/k$, since $\vert I_i\vert \geq k$. 
\end{proof}

\begin{theorem}
\claimlabelname{Bias}

\[\vert \EE[\hat{f}(x_i)]-\EE[f(x_i)]\vert \leq \frac{Lk}{n}.\]
\end{theorem}

\begin{proof}
Since $\vert f'(x_i)\vert\leq L$, the farthest neighboring point (i.e., distance $k/n$ away) differs from $f(x_i)$ by at most $Lk/n$. Therefore, the average distance from neighboring points differs from $f(x_i)$ is also bounded above by $Lk/n$.
\end{proof}

\begin{theorem}
\claimlabelname{Quadratic Risk}

Quadratic risk is the sum of variance and bias:
\[\EE[(\hat{f}(x_i)-f(x_i))^2]\leq \frac{L^2k^2}{n^2}+\frac{1}{k}.\]
\end{theorem}

So, we can minimize the quadratic risk by choosing $k=(n/L)^{2/3}$. 

\subsection{Exponential Family}

The family of exponential distributions $\PP_{\eta}$ has density function 
\[h(x)\exp(\eta^Tt(x) - a(\eta)).\]

\begin{itemize}
    \item $\eta$ and $t$ are called sufficient statistics. Note that $\eta$ is a vector of inputs to the distribution, hence the transpose
    \item $h$ is called the ``underlying measure''
\end{itemize}

$a(\eta)$ is chosen so that we get a probability distribution: 
\[a(\eta) = \log \int h(x)\exp(\eta^Tt(x))\ddd x.\]

\begin{example}
\exlabel

The normal distribution is an exponential family. 
\end{example}
Probability density can be expressed in the following way: 
\begin{align*}
    \PP_{\mu, \sigma^2}(x) &= \frac{1}{\sqrt{2\pi \sigma}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \\
    &= \exp\left(\frac{\mu}{\sigma^2}x - \frac{1}{2\sigma^2}x^2 - \frac{\mu^2}{2\sigma^2}-\log \sigma -0.5\log(2\pi)\right).
\end{align*}
\begin{itemize}
    \item $t(x) = (x,x^2)$
    \item $\eta = (\mu/\sigma^2, -1/(2\sigma^2))$
    \item $a(\eta) = \mu^2 + \log \sigma + 0.5\log(2\pi) = -\eta_1^2/4\eta_2-0.5\log(-2\eta_2)+0.5\log(2\pi)$. 
    \item $h(x) = 1$
\end{itemize}

\begin{theorem}
\claimlabel

\begin{itemize}
    \item $\displaystyle \EE_{\PP_{\eta}}[t(x)] = \nabla a(\eta)$.
    \item $\displaystyle \cov[t(x)] = \nabla^2 a(\eta)$. 
\end{itemize}
\end{theorem}