\section{February 8, 2024}

\subsection{Simple Decision Making}

Classic Exploration Exploitation setup 

The goal is to learn a policy $\pi(s_{0:t};\theta)$ that maps all the previous information into a best possible action. The objective in reinforcement learning is most generally the maximal sum of rewards over time: 
\[\max_{\theta}r_t.\] 
Compare this to the objective in normal supervised learning, which is to choose actions by maximizing a probability distribution: 
\[\max_{\theta}p(a^{gt}, s_t).\] 
If the reward function is unknown and/or not differentiable, then we have to use a different approach to attack these problems. 

\subsection{Multi-Arm Bandits}

The setup here is that we have $N$ arms corresponding to actions $a_1, \hdots, a_N$. The goal is to maximize rewards 
\[\sum_{i=1}^T r(a_i^t),\] 
where at each timestep we choose an action $a_i$ and receive reward $r(a_i)$ according to some unknown reward function. 

To measure performance, we want to know how well our model performs relative to the best possible outcome. Therefore, we are interesting in minimizing \ac{regret}: 
\[\left\lVert \sum_i r(a_i^*) - \sum_i r(a_i)\right\rVert.\] 

\subsubsection{Explore-First}

The Explore-First strategy is pretty straightforward. Up to time $K$, explore the reward function by sampling each arm equally $K/N$ times. Afterwards, choose the arm with highest average reward $\mu_i = (1/k_i)\sum_{k_i}r(a_i)$. 

Under the assumption that $r\in [0,1]$, the worst regret that this could achieve is $T$. It turns out that the asymptotic performance of this algorithm is $T^{2/3}O(N\log T)^{1/3}$. 

\subsubsection{Upper Confidence Bound (UCB)}

The idea with this algorithm is to instead create some confidence intervals about the reward function, and give an exploration bonus to actions whose confidence intervals are large. We choose actions with policy
\[a_{t+1} = \argmax_i \mu_i(t) + \sqrt{\frac{4\log t}{k_i}},\] 
where the exploration bonus here is scaling with $\sqrt{(1/k_i)}$. Roughly speaking, this upper bound on the confidence of the mean value comes from the \ac{Hoeffding Inequality}. See $18.656$ notes for details.

\subsubsection{Linear UCB}

The previous algorithm is good, but assumes that rewards are purely a function of action taken. In reality, rewards are oftentimes also a function of surrounding context; 
\[\mu(a|s) = x_a^T\theta_a\]
for some context vector $x_a$. The general approach for solving Linear UCB is as follows: 
\begin{itemize}
	\item choose a reward $\hat{r}_t^a = x_t\theta_t^a$
	\item choose the best action according to estimates $a_t\leftarrow max_a \hat{r}_t^a$
	\item repeat until timestep $t$ $\hat{R}_{0:t}^a = X_{0:t}\theta^a$
	\item solve for the best possible parameters $\min_{\theta_a}\lVert R_{0:t}^a - \hat{R}_{0:t}^a\rVert_2^2$.
\end{itemize}
The last step is a simple ridge regression, i.e., 
\[\theta_t^a = (X_{0:t}^TX_{0:t}+\lambda I)^{-1}X_{0:t}^TR_{0:t}^a.\] 

