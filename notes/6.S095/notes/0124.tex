\section{January 24, 2023}

\subsection{Stochastic Processes}

\begin{definition}
\deflabel

A \ac{stochastic process} $\{X_t\}_{t\in T}$ is a collection of random variables $X_i$, where the index $t$ is some element of an index set $T$. For continuous stochastic processes, $T$ is often $\RR_{\geq 0}$, and for the discrete processes, $T$ is often $\ZZ_{\geq 0}$. 
\end{definition}

\begin{definition}
\deflabelname{Sojourn time}

For some stochastic process $\{X_t\}_{t\in T}$, let $S=\{W_1, W_2, \hdots\}$ be the set of indices at which $X_{W_i}$ is equal to a predetermined value $K$. Each $W_m$ is called a \ac{waiting time}, and represents the duration of time between the beginning of the process and the $m$th success. Each gap between waiting times $S_m = W_m - W_{m-1}$ is called a \ac{Sojourn time}. 
\end{definition}

\subsection{Counting distributions}

\begin{definition}
\deflabel

A \ac{Bernoulli random variable} $X\sim \Bern(p)$, with $0\leq p\leq 1$, has discrete pmf $f_X(0) = 1-p$, $f_X(1) = p$. 
\end{definition}

In other words, a beroulli random variable with paramater $p$ represents a binary outcome with probability $p$ of resolving successfully. Naturally, $\EE[X] = p$ and $\var[X]=p-p^2$.

\begin{example}
\exlabel

Two weighted coins have probability $p,q$ of landing heads. After the first coin is flipped, the second coin is flipped only if the first coin was heads; otherwise, it is not flipped. Compute the variance in the outcome of the second coin. 
\end{example}

The outcome of the second coin being heads or not can be modelled by a Bernoulli random variable with parameter $pq$. Therefore, the variance is $pq(1-pq)$. 

\begin{definition}
\deflabel

A \ac{Bernoulli process} is a discrete-time stochastic process of finite or infinite i.i.d (independent identically distributed) Bernoulli random variables. 
\end{definition}

For example, a Bernoulli process may be a sequence of coin flips. The famous binomial distribution measures the number of successes in a Bernoulli process. 

\begin{definition}
\deflabel

A \ac{binomial random variable} $X\sim B(n,p)$ has discrete pmf 
\[f_X(k) = \binom{n}{k}p^k(1-p)^{n-k},\]
for $k\in \{0,\hdots, n\}$.
\end{definition}

Binomial random variables measure Bernoulli processes of length $n$, whose individual Bernoulli trials all have parameter $p$. Since each Bernoulli trial is independent, linearity of expectation and linearity of variance implies that $\EE[X] = np$, $\var[X] = np(1-p)$. 

\begin{theorem}
\lemlabel

Let $X\sim B(n,p)$ and $Y\sim B(m,p)$ be independent binomial random variables. Then $Z = X+Y$ is distributed as $B(n+m, p)$. 
\end{theorem}

\begin{proof}
\begin{align*}
    f_Z(k) &= \sum_{i=0}^{k}f_X(i)f_Y(k-i) \\ 
    &= \sum_{i=0}^k \left(\binom{n}{i}p^i(1-p)^{n-i}\right)\left(\binom{m}{k-i}p^{k-i}(1-p)^{m-k+i}\right) \\
    &= p^k(1-p)^{n+m-k}\sum_{i=0}^k \binom{n}{i}\binom{m}{k-i} \\
    &= \binom{n+m}{k}p^k(1-p)^{n+m-k}.
\end{align*}
\end{proof}

\begin{theorem}
\lemlabel

Let $X\sim B(n,p)$ and $Y\sim B(X,q)$ be independent binomial random variables. Then $Y\sim B(n, pq)$. 
\end{theorem}

\begin{proof}
Intuitively, this setup is the same as the following: flip a coin $n$ times with probability $p$ of getting heads. For each head, flip another coin with probability $q$ of getting heads. Since you only get both heads with probability $pq$, it makes sense that $Y$ is the same as $B(n, pq)$. More rigorously, 

\begin{align*}
    \PP[Y = m] &= \sum_{k=0}^n \PP[X = k]\cdot \PP[Y=m | X=k] \\
    &= \sum_{k=m}^n \binom{n}{k}p^k(1-p)^{n-k}\binom{k}{m}q^m(1-q)^{k-m} \\
    &= \sum_{k=m}^n \binom{n}{m}\binom{n-m}{k-m}p^k(1-p)^{n-k}q^m(1-q)^{k-m} \\
    &= \hdots \\
    &= \binom{n}{m}(pq)^m(1-pq)^n.
\end{align*}
\end{proof}

\begin{definition}
\deflabel

A \ac{hypergeometric} random variable $X\sim \Hgeom(N, K, n)$ models a sequence of Bernoulli trials that takes place \textit{without replacement}. This is in contrast to binomial random variables, which model Bernoulli trials \textit{with replacement}. The parameters represent the drawing of $n$ objects out of total possible $N$ different objects, where $K$ of them represent ``successful'' objects. 
\end{definition}
The discrete pmf is given by 
\[f_X(k) = \frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}.\]
This expression represents the probability that you select $k$ successful objects out of $K$ possible successful objects and $n-k$ non-successful objects out of $N-K$ possible non-successful objects. 

We also have that 
\[\EE[X] = n\cdot \frac{K}{N}\quad \text{and}\quad \var[X] = \frac{nK(N-K)(N-n)}{N^2(N-1)}.\]

\begin{example}
\exlabel

Acceptance sampling is a techniqued used for quality control. The process of acceptance sampling involves drawing smaller samples from a larger pool of objects, and accepting or rejecting the entire pool of objects based on the result of the smaller sample. For example, choosing to accept or reject a lot of $1000$ toy trucks based on a smaller sample of $100$. Given that $50$ of these trucks are defective, the probability that we see more than $5$ in our sample can be modeled by a hypergeometric distribution. 
\end{example}

\subsection{Waiting Times}

\begin{definition}
\deflabel

Geometric distrubtions model the \ac{Sojourn times} in a Bernoulli process. That is, they measure the times between consecutive successes. 
\end{definition}

\begin{definition}
\deflabel

Let $X\sim \Geom(p)$. Then the discrete pmf 
\[f_X(k) = (1-p)^{k-1}p.\]
The discrete cdf is given by
\[F_X(k) = \sum_{i=1}^k (1-p)^ip = 1-(1-p)^k.\]
\end{definition}

Note that this definition is inclusive on the first successful trial itself, i.e., $f_X(k)$ represents the probability that the first $k-1$ trials fail, and the $k$th trial is a success. 

\begin{theorem}
\lemlabelname{Memorylessness}

For geometric random variable $X$,
\[\PP[X > n+m | X > n] = \PP[X > m].\]
\end{theorem}

\begin{theorem}
\lemlabel

Let $X_1, \hdots, X_n$ be geometric random variables distributed as $X_i\sim \Geom(p_i)$. If $Y = \underset{1\leq k\leq n}{\text{min}} X_k$, then 
\[Y\sim \Geom\left(1 - \prod_{i=1}^n (1-p_i) \right)\]
\end{theorem}

\begin{proof}
The probability that at least one event happens is equal to the complement of none of them happening.
\end{proof}

\begin{definition}
\deflabel

Let $X\sim \Nbin(r,p)$ be a \ac{negative binomial} random variable. Then the discrete pmf 
\[f_X(k) = \binom{k-1}{r-1}p^{r}(1-p)^{k-r}.\]
\end{definition}

Negative binomial distributions measure the value of $W_r$, i.e., the waiting time for the $r$-th success. 

\begin{theorem}
\lemlabel

\[\Nbin(r,p) \sim \sum_{i=1}^r \Geom(p).\]
\end{theorem}

\begin{proof}
Negative binomial distributions measures total waiting times, while geometric distributions measure the time between each waiting time. By the memorylessness property of geometric distributions, Sojourn waiting times are independent.
\end{proof}

By linearity of expectation and variance (for independent random variables), this implies
\[\EE[X] = \frac{r}{p} \quad \text{ and }\quad \var[X] = \frac{r(1-p)}{p^2}\]
when $X\sim \Nbin(r,p)$.

\subsection{Continuous Time Distributions}

Here we explore the continuous analogues of binomial and geometric distributions: Poisson and Exponential distributions, respectively. 

\begin{definition}
\deflabel

A \ac{poisson} random variable $X\sim \Pois(\lambda)$ is said to have ``rate'' $\lambda > 0$ and discrete pmf 
\[f_X(k) = \frac{\lambda^k e^{-\lambda}}{k!}.\]
\end{definition}

Suppose $X\sim \Bin(n,p)$. Now fix $\lambda = np$ (recall that this is the expected number of successes over some number of trials), and increase $n$ arbitrarily large; in the limit, $n\rightarrow \infty$ and $p\rightarrow 0$. Now, 
\begin{align*}
    f_X(k) &= \lim_{n\rightarrow \infty}\binom{n}{k}p^k(1-p)^{n-k} \\
    &= \lim_{n\rightarrow \infty}\frac{n(n-1)\hdots (n-k+1)}{k!}p^k\left(1-\frac{\lambda}{n}\right)^{n-k} \\
    &= \frac{\lambda^k}{k!}\cdot e^{-\lambda},
\end{align*}
hence we recover the Poisson distribution. This is considered to be the continuous time limit of the binomial distribution in the sense that we maintain the mean number of successful trials in a given time period, but the Poisson distribution is running infinitely many trials. Given $X\sim \Pois(\lambda)$, we also have 
\[\EE[X] = \var[X] = \lambda,\]
which makes sense intuitively, since $\EE[X] = np = \lambda$ and $\var[X] = np(1-p) = \lambda$ in the limit when $n\rightarrow \infty$ and $p\rightarrow 0$. 

\begin{example}
\exlabelname{binomial approximation with poisson}

When $n$ is large, $p$ is small, and $\lambda = np$ is medium-sized, using $Y\sim \Pois(np)$ can be a good approximation for $X\sim \Bin(n,p)$. In general, it's a lot easier to calculate $f_Y(k)$ vs. $f_X(k)$ for any particular $k$.
\end{example}

\begin{theorem}
\lemlabel

Let $X_i\sim \Pois(\lambda_i)$ be independent. Then $Y = \sum_{i=1}^nX_i\sim \Pois(\sum_{i=1}^n\lambda_i)$. 
\end{theorem}

\begin{proof}
\begin{align*}
    \PP[X_1 + \hdots + X_n = k] &= \sum_{x_1 + \hdots + x_n = k}\prod_{i=1}^n\left(\frac{\lambda_i^{x_i}e^{-\lambda_i}}{x_i!}\right) \\ 
    &= \sum_{x_1+\hdots + x_n=k}\frac{e^{-\lambda_1 - \hdots - \lambda_n}}{k!}\binom{k}{x_1, \hdots, x_n}\prod_{i=1}^n \lambda_i^{x_i} \\
    &= \frac{e^{-\lambda_1 - \hdots - \lambda_n}}{k!}(\lambda_1+\hdots \lambda_n)^k.
\end{align*}
\end{proof}

\begin{definition}
\deflabel

A \ac{Poisson process} with rate $\lambda > 0$ is defined over the positive reals satisfying:
\begin{enumerate}
    \item [(1)] For any strictly increasing non-negative $t_0, t_1, \hdots, t_m$, $X(t_{i+1})-X(t_i)$ are all independent random variables. 
    \item [(2)] The random variable $X(s+t) - X(s) \sim \Pois(\lambda t)$ (for $t>0$). In words, the distribution of the number of events along any interval only depends on the length of the interval.
    \item [(3)] $X(0) = 0$.
\end{enumerate}
\end{definition}

This definition is equivalent to the following reformulation: 
\begin{definition}
\deflabel

Let $N(a,b]$ be a random variable that counts the number of events along the interval $(a,b]$. $N$ is a \ac{Poisson point process} with rate $\lambda$ if: 
\begin{enumerate}
    \item [(1)] For any strictly increasing non-negative $t_0, \hdots, t_m$, $N(t_i, t_{i+1}]$ are all independent.
    \item [(2)] $N(s,t]\sim \Pois(\lambda (t-s))$ (for $t>0$).
\end{enumerate}
\end{definition}

Some key facts about Poisson processes: 
\begin{itemize}
    \item Given the total number of events that occur along an interval, the distribution of those events along the interval is uniformly distributed. 
    \item Due to the first fact, given the number of events that occur along an interval, the number of events that occur along subintervals of that interval follows a binomial distribution. For example, if you know that $10$ events occurred $4$ times in one hour, the probability that $3$ of them occurred in the first $15$ minutes is given by $\binom{10}{3}(.25)^3(.75)^{7}$. 
    \item \ac{Poisson Splitting}: If $\{X(t)\}$ is a Poisson process with rate $\lambda$, say we ``split'' each successful event into a type $1$ event with probability $p$, and a type $2$ event with probability $(1-p)$. Then, the processes that contains only events of type $1$ is also a Poisson process with rate $\lambda p$. The process that contains only events of type $2$ is a Poisson process with rate $\lambda (1-p)$
    \item \ac{Poisson Superposition}: Let $\{X_1(t), \hdots, X_n(t)\}$ be Poisson processes with rates $\lambda_1, \hdots, \lambda_n$, respectively. The union of all of these processes is also a Poisson process with rate $\lambda_1+\hdots + \lambda_n$. 
\end{itemize}

Finally, the exponential distribution is the last distribution we cover. Like the Poisson distribution is the continuous analogue of the binomial distribution, the exponential distribution is the continuous analogue of the geometric distribution. 

\begin{definition}
\deflabel

An exponential random variable $X\sim \Exp(\lambda)$ has continuous pdf 
\[f_X(k) = \lambda e^{-\lambda k}.\]
It has cdf
\[F_X(k) = 1-e^{-\lambda k}.\]
\end{definition}



