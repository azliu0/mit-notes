\section{March 7, 2024}

\subsection{Complete Data}

Consider the following setup. We have some observed data $y$ sampled from $Y$, which has distribution $p_Y(\cdot; x)$ for some $x\in \mathcal{X}$. We want 
\[\hat{x}(y) = \argmax_{a\in \mathcal{X}}\log p_Y(y; a),\] 
i.e., the maximum likelihood hypothesis that explains the data we observed. Define $\ell_Y(a;y) = \log p_Y(y;a)$, so that
\[\hat{x}(y) = \argmax_{a\in \mathcal{X}}\ell_Y(a; y). \] 
Now suppose that there exists some latents $z$ sampled from $Z$, which has distribution $p_Z(\cdot; x)$. $Z$ is the ``complete data'', i.e., $Y = g(Z)$ for some deterministic $g$. $Z$ always exists, but we may not be able to observe it; we have to guess a form that is reasonable and hope that the results work. Given the data, we can compute an expected likelihood of our latent distribution: 
\[\EE_{Z|Y}(\cdot | y; x')[\ell_Z(x; z)],\]
for any $x'\in \mathcal{X}$. We cannot compute the true value, since we are assuming that we don't have access to the complete data (in practice, the ``complete data'' is just a guess, e.g., I assume that the data is being generated from $k$-clusters). Also, since $Y=g(Z)$ deterministically, we can say that 
\[p_Z(z;x) = p_{Z,Y}(z,y;x) = p_{Z|Y}(z|y;x)p_Y(y;x),\]
so taking the log and then expectation of both sides gives 
\[\log p_Y(y;x) = \EE_{p_{Z|Y}(\cdot | y; x')}[\log p_Z(z;x)] - \EE_{p_{Z|Y}(\cdot | y; x')}[\log p_{Z|Y}(z|y; x)]\] 
for all $x,x'\in \mathcal{X}$. Re-write this term wise as 
\[\ell_Y(x;y) = U(x;x') + V(x;x').\] 
By Gibbs, $V(x;x')\geq V(x';x')$, so this rearranges to 
\begin{align*}
\ell_Y(x;y)\geq (U(x;x')-U(x';x')) + U(x';x')+V(x';x') \geq \Delta(x,x') + \ell_Y(x';y).
\end{align*} 
Given that we choose $x$ s.t. $\Delta(x,x') > 0$, we now have $\ell_Y(x;y) > \ell_Y(x';y)$, which gives a way to guarantee an increase in log likelihood. This is the foundation for the EM-algorithm.

\subsection{Expectation-Maximization Algorithm}

The EM-algorithm is as follows: 
\begin{itemize}
	\item Initialize $t=0$ and a guess for $\hat{x}^{(0)}$. 
	\item \textbf{E}: Compute 
		\[U(x;\hat{x}^{(t)}) = \EE_{p_{Z|Y}(\cdot | y; \hat{x}^{(t)})}[\log p_Z(z;x)].\] 
	\item \textbf{M}: Compute 
		\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}U(x; \hat{x}^{(t)}).\]
	\item Increment $t$ and repeat the \textbf{EM} cycle until convergence. 
\end{itemize}
The intuition behind the \textbf{M}-step here is to guarantee that $\Delta(\hat{x}^{(t+1)}, \hat{x}^{(t)}) \geq 0$, so that we have an increasing (non-decreasing) sequence of likelihoods 
\[\{\ell_Y(\hat{x}^{(t)})\}_{t\geq 0}.\] 
The hope is that after enough steps, we can converge on a optimal value. 

\subsection{EM on Logistic Regression}

The goal of linear regression is to assign a probability $\sigma(\theta^Tx)$ that the assigned label of a data point is $1$. Suppose we have complete data 

\[\mathcal{D}_+ = \{(u_i,v_i,w_i)\}_{1\leq i\leq N},\] 
where the hidden part of the data is a mixture component $w_i\in \{0,1\}$. The observed data is 

\[\mathcal{D} = \{(u_i,v_i)\}_{1\leq i\leq N},\]
which are the standard pairs of vectors $u_i$ and labels $v_i\in \{\pm 1\}$ for logistic regression. For the \mathbf{E} step, we are interested in $p_Z(z)$, which is governed by

\[p_{V,U,W}(v,u,w) = p_{V|U,W}(v|u,w)p_U(u)p_W(w).\] 
We have 
\[p_{V|U,W}(v|u,w) = \frac{1}{1 + e^{-vx_w^Tt(u)}},\] 
where $t(u)$ is a feature vector extracted from $u$, and $x_w$ is the learned hypothesis vector given mixture $w$. If we suppose that $w\sim \Bern(q)$ for unknown $q$, then we also have 
\[p_W(w) = \exp\left(w\ln \left(\frac{q}{1-q}\right) + \ln (1-q)\right),\] 
so
\[\ln p_Z(z) = -\ln(1+e^{-vx_w^Tt(u)}) + \ln p_U(u) + w\ln\left(\frac{q}{1-q}\right) + \ln (1-q).\] 
The full hypothesis that we want to learn is $\theta = (x_0, x_1, q)$. Now we can compute the expectation. For the first term,
\begin{align*}
	&\EE_{p_{Z|Y}(\cdot | y; \theta')}[-\ln (1 + e^{-vx_w^Tt(u)})] \\
	&\qquad = -p_{W|U,V}(0|u,v;\theta')\ln(1+e^{-vx_0^Tt(u)}) - p_{W|U,V}(1|u,v;\theta')\ln(1 + e^{-vx_1^Tt(u)}).
\end{align*}
For the last term,
\begin{align*}
	\EE_{p_{Z|Y}(\cdot | y; \theta')}[w\ln \left(\frac{q}{1-q}\right) + \ln (1-q)] = p_{W|U,V}(1|u,v;\theta')\ln\left(\frac{q}{1-q}\right) + \ln(1-q),
\end{align*}
so the whole expectation is
\begin{align*}
	U(\theta, \theta') &= \EE_{p_{Z|Y}(\cdot | y; \theta')}[\sum_{i=1}^N\ln p_Z(z_i)] \\
										 &= -\sum_{i=1}^N p_{W|U,V}(0|u_i,v_i;\theta')\ln(1 + e^{-v_ix_0^Tt(u_n)}) \\
										 &\qquad -\sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta')\ln(1 + e^{-v_ix_1^Tt(u_n)}) \\
										 &\qquad + \sum_{i=1}^N\ln p_U(u_i) + \sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta')\ln \left(\frac{q}{1-q}\right) + N \ln(1-q).
\end{align*}
To calculate each of the posteriors, we can use Bayes': 
\begin{align*}
	&p_{W|U,V}(1|u_i,v_i;\theta')\\
	&\qquad = \frac{p_{W}(1;\theta')p_{V|U,W}(v_i|u_i,w_i=1;\theta')}{p_{W}(0;\theta')p_{V|U,W}(v_i|u_i,w_i=1;\theta') + p_W(1;\theta')p_{V|U,W}(v_i|u_i,w_i=1;\theta')} \\
	&\qquad= \frac{q'(1 + \exp(-v_i(x_1')^Tt(u_i)))^{-1}}{(1-q')(1 + \exp(-v_i(x_0')^Tt(u_i)))^{-1} + q'(1 + \exp(-v_i(x_1')^Tt(u_i)))^{-1}} \\
	&\qquad= \left(1 + \frac{1-q'}{q'}\frac{1 + \exp(-v_i(x_1')^Tt(u))}{1 + \exp(-v(x'_0)^Tt(u))}\right)^{-1}.
\end{align*}
For the $\mathbf{M}$ step, we can optimize each component in $\theta=(x_0,x_1,q)$ separately. In particular, we can take 
\[x_i^{(t+1)} = \argmin_{x} \sum_{i=1}^N \ln(1 + e^{-v_ix^Tt(u_i)})\]
and 
\[q^{(t+1)} = \argmax_{q} \sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta')\ln \left(\frac{q}{1-q}\right) + N\ln(1-q).\] 
The posteriors rely only on $q'$, so they are constant relative to our maximization. We can solve directly for the maximization for $q^{(l+1)}$: 
\begin{align*}
&\left(\frac{1}{N}\sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta')\right)\ln\left(\frac{q}{1-q}\right) + \ln(1-q) \\
&\qquad = \left(\frac{1}{N}\sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta')\right) \ln (q) + \left(1 - \frac{1}{N}\sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta')\right)\ln (1-q). 
\end{align*}
This the expected value of log Bernoulli $\ln q$ with respect to another Bernoulli, so by Gibbs,
\[q^{(t+1)} = \frac{1}{N}\sum_{i=1}^N p_{W|U,V}(1|u_i,v_i;\theta').\] 

\subsection{EM on Infectious Disease}

Say we have a population of $N$ people over $T$ years, and data $y_i\in \{0,1\}$ signalling whether a subject $i\in [N]$ contracted an infectious disease by te end of the $T$ years. We have access to a record $\mu_{ij}\in \{0,1\}$ that tells us whether or not subject $i\in [N]$ was exposed to the disease in year $j\in [T]$. Our goal is to estimate the infection rate $r_j$ for each year $j\in [T]$. 

Naturally, the ``complete'' data that explains our observations can be represented by $z_{ij}\in \{0,1\}$ indicating whether $i$ contracted the disease during year $j$. We don't have access to this data, but we can use the EM algorithm on this latent space to estimate $r_j$. 

\V

\noindent We can show using a bit of casework that 
\[p_{z_{ij}}(z_{ij};r) = r_j^{\mu_{ij}z_{ij}}(1-r_j)^{\mu_{ij}z_{ij}},\] 
so 
\[\ln p_Z(z;r) = \sum_{i=1}^N\sum_{j=1}^T z_{ij}\mu_{ij}\ln r_j + \mu_{ij}(1-z_{ij})\ln (1-r_j).\] 
Now we perform $\mathbf{E}$: 
\begin{align*}
	&\EE_{p_{Z|Y}(\cdot | y;r')}\left[\sum_{i=1}^N\sum_{j=1}^Tz_{ij}\mu_{ij}\ln r_j + \mu_{ij}(1-z_{ij})\ln (1-r_j)\right] \\
	&\qquad\qquad= \sum_{j=1}^T\sum_{i=1}^N \mu_{ij}\EE_{p_{Z|Y}(\cdot | y;r')}[z_{ij}]\left(\frac{r_j}{1-r_j}\right) + \mu_{ij}\ln (1-r_j).
\end{align*}
We also need the expectation for each $z_{ij}$: 
\begin{align*}
	\EE_{p_{Z|Y}(\cdot | y;r')}[z_{ij}] = \PP[z_{ij}=1|Y=y,r'] = \frac{\mathbbm{1}(y_i=1)\mu_{ij}r_j'}{\sum_{j'=1}^T\mathbbm{1}(y_i=1)\mu_{ij'}r_{j'}'}.
\end{align*}
To perform $\mathbf{M}$, note that we can optimize one component at a time by taking the inner sum, i.e., 
\[r_j = \argmax_{r_j}\left(\sum_{i=1}^N\mu_{ij}\EE_{p_{Z|Y(\cdot | y;r')}}[z_{ij}]\left(\ln \frac{r_j}{1-r_j}\right) + \mu_{ij}\ln (1-r_j)\right).\] 
Taking critical points gives
\[\sum_{i=1}^N\left(\mu_{ij}\EE_{p_{Z|Y(\cdot | y;r')}}[z_{ij}]\cdot \frac{1-r_j}{r_j}\frac{(1-r_j) + r_j}{(1-r_j)^2} - \frac{\mu_{ij}}{1-r_j}\right) = 0,\] 
which gives
\[r_j = \frac{\sum_{i=1}^N\left(\mu_{ij}\cdot \frac{\mathbbm{1}(y_i=1)\mu_{ij}r_j'}{\sum_{j'=1}^T\mathbbm{1}(y_i=1)\mu_{ij'}r_{j'}'}\right)}{\sum_{i=1}^N\mu_{ij}}.\] 
Note that this result intuitively makes sense; we want $r_j$ to approach the true proportion $\EE[z_{ij}]$ of infected subjects, and we weight only by $\mu_{ij}=1$ because we can say definitively that $z_{ij}=1$ should only happen when subjects are exposed.

\subsection{EM Alternate Formulation}

Here we introduce an alternate formulation for the EM algorithm that we introduced earlier. Say the complete data is $Z\in \mathcal{Y}\times \mathcal{S}$, where $Y\in \mathcal{Y}$ is our observed data and $S\in \mathcal{S}$ is the latent. Consider the objective function 
\[\ln \tilde{p}_{Y}(y;x,q_{S|Y}(\cdot|y)) = \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y,S}(y,s;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)],\] 
for all $x\in \mathcal{X}$ and $y\in \mathcal{Y}$. This is similar to the original setup, but now the conditioning on $x$ no longer affects the distribution over the expected values, and moreover the distribution over the expected values is an additional input to our likelihood function. I'm not sure what the significance of these differences are yet, but this setup will supposedly be important when we learn about variational inference. 

Since we now have two inputs conditioning our likelihood function, we can alternate between them for our algorithm. First, guess $\hat{x}^{(0)}$, and then alternate
\[\hat{q}_{S|Y}^{(t)}(\cdot | y) = \argmax_{q\in \mathscr{P}_+^{\mathcal{S}}}\ln \tilde{p}_Y(y;\hat{x}^{(t)}, q_{S|Y}(\cdot | y)),\] 
\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}\ln \tilde{p}_Y(y;x,\hat{q}_{S|Y}^{(t)}(\cdot | y)).\] 
We will show that this is equivalent to the original EM formulation. First, note that 
\begin{align*}
	\ln \tilde{p}_{Y}(y;x,q_{S|Y}(\cdot|y)) &= \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y,S}(y,s;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)] \\
																					&= \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y}(y;x)] + \EE_{q_{S|Y}(\cdot | y)}[\ln p_{S|Y}(\cdot | y;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)] \\
																					&\leq \ln p_Y(y;x) + \EE_{q_{S|Y}(\cdot | y)}\ln q_{S|Y}(\cdot|y;x) - \EE_{q_{S|Y}}\ln q_{S|Y}(\cdot | y;x) = \ln p_Y(y;x), 
\end{align*}
by Gibbs. We have equality when $q_{S|Y}(\cdot | y) = p_{S|Y}(\cdot | y;x)$, so 
\[q^{(t)}_{S|Y}(\cdot | y) = p_{S|Y}(s|y;x^{(t)}).\] 
Now,
\begin{align*}
	\ln (\tilde{p}_y;x,q_{S|Y}^{(t)}(\cdot | y)) &= \EE_{p_{S|Y}(\cdot|y;x^{(t)})}[\ln p_{Y,S}(y,s;x)] - \EE_{p_{S|Y}(\cdot|y;x^{(t)})}[\ln p_{S|Y}(s|y)].
\end{align*}
The second term on the RHS is not a function of $x$, so it doesn't affect our $\argmax$. Since $Z=(Y,S)$, we thus have 
\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}\EE_{p_{Z|Y}(\cdot | y;x^{(t)})}[\ln p_{Z}(z;x)] = U(x;\hat{x}^{(t)}),\]
which recovers the original EM form, so we are done.


