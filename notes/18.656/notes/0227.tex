\section{February 27, 2024}

Outline for today: 
\begin{itemize}
	\item Martingales and Azuma-Hoeffding
	\item Some examples 
	\item Rademacher and Gaussian complexities
	\item Gauss-Lipschitz functions
\end{itemize}

\subsection{Recap}

Let
\[D_i = \EE[f(X_1)^n|X_1,\hdots,X_i] - \EE[f(X_1^n) | X_1,\hdots, X_{i-1}].\] 
By telescoping,
\[f(X_1^n) - \EE[f(X_1^n)] = \sum_{i=1}^nD_i,\]
and the Martingale difference property says that 
\[\EE[D_i|X_1,\hdots,X_{i-1}]=0.\] 

\subsection{Idk}

\begin{theorem}
\claimlabel 

Say that we have 
\[\EE[e^{\lambda D_i}|X_1,\hdots,X_{i-1}]\leq e^{\lambda^2 \nu_i^2/2}\quad \forall \vert\lambda\vert < \frac{1}{\alpha_i},i\in [n].\]
Then, we have 
\[\EE[e^{\lambda \sum_{i=1}^n D_i}]\leq e^{\lambda^2/2\cdot \left(\sum_{i=1}^n \nu_i^2\right)}\quad \forall \vert \lambda\vert \leq \frac{1}{\max_{i\in [n]}\alpha_i}.\]
\end{theorem}
\begin{proof}
\begin{align*}
	\EE[e^{\lambda \sum_{i=1}^n D_i}] &= \EE[e^{\lambda \sum_{i=1}^{n-1}D_i}e^{\lambda D_n}] \\
																		&= \EE_{X_{[1...n-1]}}[\EE_{X_n}[e^{\lambda \sum_{i=1}^{n-1}D_i}e^{\lambda D_n}|X_1,\hdots, X_{n-1}]] \\
																		&= \EE_{X_{[1...n-1]}}[e^{\lambda \sum_{i=1}^{n-1}D_i}\EE_{X_n}[e^{\lambda D_n}|X_1,\hdots,X_{n-1}]]. 
\end{align*}
By assumption, this quantity is bounded above by 
\[\EE_{X_1^{n-1}}[e^{\lambda \sum_{i=1}^{n-1}}]e^{\lambda^2\nu_n^2/2},\]
for all $\vert \lambda\vert \leq \frac{1}{\alpha_n}$. If we keep applying this inductively, we have the result. 
\end{proof}
\begin{theorem}
\thmlabelname{Azuma-Hoeffding Bound}

Consider a special case, when $D_i\in [a_i,b_i]$ almost surely. Then,

\[\PP\left(\sum_{i=1}^n D_i\geq t\right)\leq e^{-2t^2/(\sum_{i=1}^n (b_i-a_i)^2)}\quad \forall t > 0.\] 
\end{theorem}

Suppose $f$ satisfies bounded-differences: 
\[\vert f(x_1,\hdots,x_{i-1},x_i,\hdots,x_n) - f(x_1,\hdots,x_{i-1},\tilde{x_i},\hdots,x_n)\vert\leq c_i\quad \forall x_i,\tilde{x_i}.\] 
Then,
\[\vert D_i\vert = \vert\EE[f(X_1^n)|X_1,\hdots,X_i] - \EE[f(X_1^n)|X_1,\hdots,X_{i-1}]\vert \leq c_i\]
since expected values are just linear combinations of functions \comment{I'm confused?}. We can apply the Azuma-Hoeffding bound with $a_i=-c_i$ and $b_i=c_i$ to get the following claim.   
\begin{theorem}
\claimlabel

If $f$ satisfies $(c_i)_{i=1}^n$-bounded-differences,
\[\PP[f(X_1^n) - \EE[f(X_1^n)]\geq t]\leq e^{\frac{-2t^2}{4\sum_{i=1}^nc_i^2}} = e^{\frac{-t^2}{2\sum_{i=1}^n c_i^2}}\quad \forall t>0.\] 
\end{theorem}
\comment{look at this example later}
\begin{example}
\exlabel

See book for concentration of clique number in a random graph model. Something about Vertex martingales, Edge martingales. 
\end{example}

\subsection{Noise Complexities}
Suppose we observe some data 
\[Y=\Theta^*+W,\] 
where $\Theta^*\in \mathcal{C}\subseteq \RR^n$ is an unknown object. We want to be able to recover $\Theta^*$ given noise $W\in \RR^n$. Say that the noise components are i.i.d. satisfying $\EE[w_i]=0$ and $\var[w_i]=\sigma^2$. Our goal is to recover a least-squares estimate 
\[\hat{\Theta} = \argmin_{\Theta\in \mathcal{C}}\frac{1}{n}\lVert Y-\Theta\rVert_2^2.\] 
Now let's say we have another fresh sample 
\[\tilde{Y} = \Theta^* + \tilde{W},\] 
which could represent a holdout dataset or something similar. We are interested in an empirical risk function defined on this holdout data: 
\[L(\theta) = \frac{1}{n}\EE[\lVert \tilde{Y}-\theta\rVert_2^2].\] 
With some math, \comment{??}, we can show that 
\[L(\theta) = \frac{1}{n}\lVert \theta - \theta^*\rVert + \sigma^2.\] 
We can measure the quality of our procedure with the following quantity: 
\[L_n(\hat{\theta}) - L(\hat{\theta}). \] 
If this quantity is too large, then we are overfitting, because the empirical risk generated by our new samples is large compared to expected risk. We can expand this quantity further: 
\begin{align*}
	L_n(\hat{\theta}) &= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{\theta}_i)^2 \\
															&= \frac{1}{n}\sum (\theta_i^* - \hat{\theta}_i + w_i)^2 \\
															&= \frac{1}{n}\sum (\theta_i^* - \hat{\theta}_i)^2 + \frac{1}{n}\sum_{i=1}^n w_i^2 - \frac{2}{n}\sum_{i=1}^n w_i(\hat{\theta} - \theta^*). 
\end{align*}
When we subtract $L(\hat{\theta})$, 
\[L_n(\hat{\theta}) - L(\hat{\theta}) = \left(\frac{1}{n}\sum_{i=1}^n w_i^2 - \sigma^2\right) - \frac{2}{n}\sum_{i=1}^n w_i(\hat{\theta} - \theta^*).\] 
The first part of this expression (in parenthesis) is an i.i.d. zero-mean tail bound, which we know how to bound effectively. The worst case of over-fitting is when we have 
\[\hat{\theta} = Y = \theta^* + W,\]
in which case the last term becomes very large: 
\[-\frac{1}{n}\sum_{i=1}^n w_i(\hat{\theta_i} - \theta_i^*) = -\frac{1}{n}\sum_{i=1}^n w_i^2.\] 
The quantity $L(\hat{\theta})$ is called that \ac{population risk}, while $L_n(\hat{\theta})$ is called the \ac{empirical risk}. 
\[L(\hat{\theta}) \leq L_n(\hat{\theta}) + \left\vert \frac{1}{n}\sum_{i=1}^n w_i^2 - \sigma^2\right\vert + \sup_{\theta\in \mathcal{C}}\left\vert\frac{1}{n}\sum_{i=1}^n w_i(\theta_i-\theta_i^*)^2\right\vert\]
The rightmost term is known as the \ac{noise complexity} of the set 
\[a = \{\theta-\theta^*|\theta\in \mathcal{C}\}. \] 

\subsection{Rademacher and Gaussian complexities}
In general noise complexity, the only guarantees that we made about the errors were that they were zero-main with variance $\sigma^2$. We could focus on more specific distributios of error to obtain more specific complexities. 

Let $a\subseteq \RR^n$. Then define 
\[Z(a) = \sup_{a\in a}\left\vert \frac{1}{n}\sum_{i=1}^n a_i\varepsilon_i\right\vert,\] 
where $\varepsilon_i\in \{\pm 1\}$ are i.i.d. Rademacher. The inner sum can also be written more succinctly as the inner product $\langle a,\varepsilon\rangle$.
\[R(a) = \EE_{\varepsilon}[Z_n(a)].\] 
We could replace the inner $\varepsilon$ with any type of random variable, like Gaussians, which would give a Gaussian complexity. 
\begin{example}
\exlabel

$L_p$ balls. 
\end{example}

\noindent Define our noise set 
\[a = \BB_p(1) = \left\{a \big| \left(\sum_{j=1}^n \vert a_j\vert^p\right)^{1/p}\leq 1\right\}.\] 
Then 
\[R(\BB_1(1)) = \sup_{\lVert a\rVert_1\leq 1}\frac{1}{n}\langle a,\varepsilon \rangle \leq \frac{1}{n} \sup_{\lVert a\rVert_1\leq 1} \lVert a\rVert_1\lVert \varepsilon\rVert_{\infty} = \frac{1}{n},\]
by Holder's inequality. 
We also have 
\[R(\BB_2(1)) = \sup_{\lVert a\rVert_2\leq 1}\frac{1}{n}\langle a,\varepsilon \rangle \leq \frac{1}{n} \sup_{\lVert a\rVert_2\leq 1} \lVert a\rVert_2\lVert \varepsilon\rVert_{2} = \frac{1}{\sqrt{n}},\]
by Cauchy Schwarz. It is left as an exercise to show that
\[R(\BB_{\infty}(1)) = 1.\]
\comment{look at this later}
