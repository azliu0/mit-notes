\section{February 7, 2023}

\subsection{Administration}

Problem sets are hard. Start early, don't wait until the last minute (in fact, there will be no office hours on the day that problem sets are due, in order to discourage this practice). The worst two problem sets are dropped. You are allowed to turn in problem sets up to $2$ days late, at most $3$ times, with no penalty. Collaboration on problem sets is highly encouraged.

Prof. Madry calls this class the ``Art and Craft'' of Algorithms. In this class, the content covered will generally fall into three different themes: 
\begin{itemize}
    \setlength \itemsep{0.01cm}
    \item Techniques
    \item Models of Computation 
    \item Intractability
\end{itemize}

\subsection{Intro to Complexity}

We will learn more about this throughout the semester. Algorithms that can be solved in $O(n^c)$ for some constant $c$ are said to be efficient. 

\begin{definition}
\deflabelname{P}

$P$ is the set of all decision problems that can be solved in $O(n^c)$. 
\end{definition}

For example, the \ac{Eulerian cycle problem}: given a graph, is it possible to find a cycle which traverses every edge of the graph exactly once?

\begin{definition}
\deflabelname{NP}

$NP$ is the set of all decision problems that can be verified in $O(n^c)$.
\end{definition}

$NP$ does not stand for ``not polynomial''; instead, it stands for non-deterministic polynomial time. 

As an example, the \ac{Hamiltonian cycle problem} is NP-complete: given a graph, is it possible to find a cycle which traverses every vertex of the graph exactly once? NP-completeness means that it is both in NP (since it is easy to verify that a valid cycle is in fact valid), and it is NP-hard (approximately, it is as hard as all other problems in NP). We will work with these ideas more formally in later lectures. 

\subsection{Scheduling (greedy)}

We have one resource, and $n$ requests for this resource $R = \{r_1, \hdots, r_n\}$. Each request corresponds to some start and finish time, i.e., $r_i = [a_i, b_i]$. For each request $r$, define its set of incompatible requests as
\[\text{Inc}(r) = \{r' | r\cap r' \neq \emptyset\}\]
Given $R$, compute the maximal set of compatible requests. 

\hrulebar

A typical greedy approach is structured like this: 
\begin{itemize}
\setlength \itemsep{0cm}
    \item Use a simple (myopic) rule to pick $r_i$
    \item Include $r_i$ in our solution, and remove everything in the set $\text{Inc}(r_i)$. 
    \item Repeat until there are no more requests remaining.
\end{itemize}

It remains to figure out what simple rule we should use. Let's consider a few candidates. 
\begin{itemize}
\setlength \itemsep{0cm}
\item Remove the request with the shortest length: this does not work. For a counter example, consider $R = \{[1,3], [3,5], [2.5, 3.5]\}$. The shortest interval can still kill multiple longer intervals that do not overlap. 
\item Remove the request with the earliest start time: this does not work. For a counter example, consider $R = \{[1,5], [2,3], [4,5]\}$. The earliest request can kill everything else. 
\item Remove the request with the smallest number of incompatible requests: this does not work. For a counter example, consider 
\[R = \{[1,2], [2,3], [3,4], [4,5], [1.5, 2.5], [1.5, 2.5], [2.5, 3.5], [3.5, 4.5], [3.5, 4.5]\}.\] 
\item Remove the request with the earliest end time: this works. 
\end{itemize}

\begin{theorem}
\lemlabel

The greedy algorithm while always removing the request with earliest end time produces an optimal solution.
\end{theorem}

\begin{proof}
Assume otherwise. Let $S$ be our greedy scheduling, and $S'$ be an optimal scheduling. Let $s_1, \hdots$ be the greedy schedule, in order, and $s_1', \hdots$ be the optimal schedule, in order. Let $s_i = [a_i, b_i]$ be the first task at which the two schedules differ (if the two schedules are the same, we are done). We must have $b_i < b_i'$ by our greedy algorithm. Therefore, we could replace $s_i'$ with $s_i$ and not decrease the number of scheduled tasks in the optimal solution. This implies $\vert S\vert \leq \vert S'\vert$, which means that the greedy solution is also optimal. 
\end{proof}

\subsection{Scheduling (DP)}

Now, imagine that we have the same problem, but tasks are weighted. Our new goal is to schedule tasks so that the weight is maximal. 

\hrulebar

We can use dynamic programming. Sort the tasks by start time, so that $a_1\leq a_2\leq \hdots \leq a_n$. Let $\textsc{Opt}(R)$ be weight of the best schedule given $R$. The recurrence is given by
\[\textsc{Opt}(R) = \max\{\textsc{Opt}(R-r), w_r + \textsc{Opt}(R-\text{Inc}(r))\},\]
with base case $\textsc{Opt}(R)=0$ when $R=\emptyset$. Computing $\text{Inc}(r)$ naively at each step gives a runtime of $O(n^2)$. This can be sped up using binary search to give a runtime of $O(n\log n)$. 

