\section{February 16, 2023}

\subsection{Union Find}

The goal of this data structure is to maintain a dynamic collection of pairwise disjoint sets $S = \{s_1, \hdots, s_r\}$ with a single arbitrary representative per set, $Rep[s_i]$. Our goal is to be able to support these three methods: 
\begin{itemize}
    \item \textsc{Make-Set}$(x)$: add set $\{x\}$ to the set of sets, with $x$ as a representative. This is an initialization method.
    \item \textsc{Find-Set}$(x)$: if $s(x)$ is the set containing element $x$, this method returns $Rep[s(x)]$.
    \item \textsc{Union}$(x,y)$: let $s(x)$ and $s(y)$ be the two sets containing $x$ and $y$ (possibly equal). This method replaces both sets with $s(x)\cup s(y)$ with a single representative.
\end{itemize}

\begin{example}
\exlabel

Use doubly-linked lists to implement this data structure. 
\end{example}

\begin{itemize}
    \item \textsc{Make-Set}(x): Runtime: $O(1)$. 
    \item \textsc{Find-Set}(x): Iterate through $s(x)$ until the leader is found. Runtime: $O(L)\in O(n)$, where $L$ is the length of $s(x)$.
    \item \textsc{Union}(x,y): Link together $s(x)$ and $s(y)$. Runtime: $O(L)\in O(n)$.
\end{itemize}

Consider the following scenario: we call $\textsc{Make-Set}$ for $n$ elements $a_1, \hdots, a_n$. Then, we call $\textsc{Union}(a_1, a_2), \textsc{Union}(a_1, a_3), \hdots, \textsc{Union}(a_1, a_n)$. Since we are traversing a list of growing size, the total runtime of these $n$ operations is 
\[\Theta(1) + \hdots + \Theta(n) = \Theta(n^2).\]

\begin{example}
\exlabel

Optimization $1.1$: create a pointer from each element to the head of the set. By maintaining these pointers, we can retrieve leaders in constant time. 
\end{example}

This changes the runtime of $\textsc{Find-Set}(x)$ to constant time. However, $\textsc{Union}(x,y)$ remains bad, since we have to replace the ``leader'' pointers for each element in the merged list. 

\begin{example}
\exlabel

Optimization $1.2$: maintain the size of each set. During the $\textsc{Union}$ operation, only join lists that are shorter to lists that are longer. 
\end{example}

\begin{theorem}
\lemlabel

Using this optimization, the amortized time of $\textsc{Union}$ is $O(\log n)$. In other words, any sequence of $m\geq n$ operations that contains $k$ union operations runs in time $O(m + k\log n)$. 
\end{theorem}

\begin{proof}
Focus on a single element $u$. Each time it is merged with another element $x$, we only update its leader (which comes with cost $O(L)$) when $\text{Length}(x)\geq \text{Length}(u)$, i.e., $\text{Length}(u)$ doubles. If we merge $u$ with a smaller element, it comes with no cost associated with the element $u$. Since we can only double the length of the set containing $u$ at most $\log n$ times, the total cost of all union operations is bounded by $O(n\log n)$.
\end{proof}

Let's change our abstraction. Instead of thinking of sets as doubly-linked lists, let's think of them as trees. For the union operation, update the ``leader'' pointer of only the leader of the merged set. For the find operation, traverse upwards until the leader is found. 

\begin{example}
\exlabel

Optimization $2.1$: Define the \ac{rank} of a leader $Rep[s(x)]$ to be its number of children. Then, only join trees with smaller ranks to trees with larger ranks. 
\end{example}

\begin{example}
\exlabel

Optimization $2.2$: \ac{path compression}. During each \textsc{Find-Set} operation, re-direct the parent pointer of each visited node to the leader. 
\end{example}

The idea behind this optimization is to flatten the tree. Each time we call \textsc{Find-Set}, we take all traversed nodes and reposition them so that they are adjacent to the leader node. \V

\begin{algorithm}[H]
\DontPrintSemicolon
\SetNoFillComment

\caption{\textsc{Find-Set}}\label{alg:findset}
\If {$x \neq x.parent$}{
    $x.parent = \textsc{Find-Set}(x.parent)$\;
}
\Return{$x.parent$}
\end{algorithm}

\begin{algorithm}[H]
\DontPrintSemicolon
\SetNoFillComment

\caption{\textsc{Union}}\label{alg:union}
$\ov{U} = \textsc{Find-Set}(U)$\;
$\ov{V} = \textsc{Find-Set}(V)$\;
\If {$\ov{U} = \ov{V}$} \Return
\uIf {$\ov{U}.rank = \ov{V}.rank$}{
    $\ov{U}.rank = \ov{U}.rank + 1$\;
    $\ov{V}.parent = \ov{U}$\;
} \uElseIf {$\ov{U}.rank > \ov{V}.rank$}{
    $\ov{V}.parent = \ov{U}$\;
} \Else {
    $\ov{U}.parent = \ov{V}$
}
\end{algorithm}
\begin{itemize}
    \item Claim 1: Optimization $2.1$ gives $O(\log n)$ $\textsc{Union}$ and $\textsc{Find-Set}$ operations. The proof for this is the same as it was for the linked-list representation. 
    \item Claim 2: Optimization $2.2$ gives $O(\log n)$ amortized. We will prove this when we discuss amortized analysis in a bit. (Example \hyperlink{pathcompression}{4.10}).
    \item Claim 3: Both optimizations together produces $O(\alpha(n))$, which is the \ac{inverse ackermann} function. This can essentially be thought of as constant time. For example, $\alpha(10^{80})\leq 4$. We will not be proving this. 
\end{itemize}

\subsection{Amortized Analysis (Queue using Two Stacks)}

A queue can be implemented using two stacks, $s_1$ and $s_2$, in the following way: 
\begin{itemize}
    \item \textsc{Enqueue(x)}: push $x$ onto $s_1$
    \item \textsc{Dequeue()}: If $s_2$ is empty, pop elements in $s_1$ until $s_1$ is empty, then push them onto $s_2$. Then, pop a single element off of $s_2$.
\end{itemize}

The \textsc{Enqueue} operation is always constant. The \textsc{Dequeue} operation is not always constant, e.g., when $s_1$ is full, but is constant amortized. Here are three ways to show that the amortized cost is constant. 

\begin{definition}
\deflabel

\ac{Aggregated} amortized time analysis is the most straightforward to understand (but often hard to compute in practice). It computes the total cost of $n$ operations by directly summing the cost of each operation.
\end{definition}

Consider the cost of the $i$th call to \textsc{Dequeue}. Suppose $n_i$ is the total number of calls to \textsc{Enqueue} that were made before the current \textsc{Dequeue}. The total cost is at most $O(n_i)$, since it takes $O(n_i)$ to pop everything off of $s_1$, $O(n_i)$ to push everything onto $s_2$, and possibly some extra cost to pop off the element itself. Note that the total number of enqueues is $O(n)$, so the total cost of all dequeues is $O(n_1+\hdots +n_k) \in O(n)$. Therefore, the amortized cost of \textsc{Dequeue} is $O(1)$. 

% \begin{definition}
% \deflabel

% The \ac{accounting} amortized time analysis
% \end{definition}

\begin{definition}
\deflabel

The \ac{potential} amortized time analysis assigns each operation with a new cost via a potential function. The main insight is that if we can choose a potential function to ``balance out'' expensive operations, our newly assigned costs can all be bounded by our desired amoritzed cost.  
\end{definition}

For each of the $n$ operations, let their actual costs $c_1, \hdots, c_n$. Our goal is to compute the amortized cost $\sum c_i / n$. For each operation, defined its amortized cost to be
\[\hat{c_i} = c_i + \Phi(D_i) - \Phi(D_{i-1}),\]
where $\Phi$ is a potential function mapping each state of the process $D_i$ to a real number. Note that 
\[\sum \hat{c_i} = \sum c_i + (\Phi(D_n) - \Phi(D_0)).\]
Therefore, as long as $\Phi(D_i)\geq \Phi(D_0)$ for all $i$, we can say that $\sum \hat{c_i}\geq \sum c_i$, and compute an upper bound on the actual total cost of all operations.

As some general intuition, our goal is to pick a potential function that makes $\hat{c_i}$ as close as possible to the actual amortized cost that we want to prove. For example, in the queue with two stacks example, we want to show that dequeue is $O(1)$ amortized, so we want to pick a potential function such that all $\hat{c_i}$ are constant time. We know that the most expensive operations for dequeue are related to the number of elements in $s_1$, so we are motivated to choose a potential function that offsets this heavier cost.

\begin{example}
\exlabel

Proof that the \textsc{Dequeue} method runs in $O(1)$ amortized, using the potential method.
\end{example}

Let $\Phi(D_i)$ be $2\vert s_1\vert$ after the $i$th operation has completed. For each call to \textsc{Enqueue}, the actual cost is $1$ for a simple push operation, and the size of $s_1$ increases by $1$ element, so 
\[\hat{c_i}^{\textsc{Enqueue}} = 1 + \Delta \Phi = 3.\]
For each call to \textsc{Dequeue}, the actual cost is $2\vert s_1\vert+1$, since it costs $\vert s_1\vert$ to pop every element in $s_1$, $\vert s_1\vert$ to push them all back onto $s_2$, and an additional $1$ to pop the last element from $s_2$. Also, the change in potential is $-2\vert s_1\vert$. So, 
\[\hat{c_i}^{\textsc{Dequeue}} = 2\vert s_1\vert + 1 + \Delta \Phi = 1.\]
In both cases, $\hat{c_i}$ is constant. Moreover, since $\Phi(D_0) = 0$, it must be the case that $\Phi(D_i)\geq \Phi(D_0)$ for all $i$. Together, this implies that the sum of the actual costs is $O(n)$, hence we have $O(1)$ amortized as desired.

\hypertarget{pathcompression}{}
\begin{example}
\exlabel

Proof that path compression results in $O(\log n)$ amortized time for both \textsc{Find-Set} and \textsc{Union} in the union-find data structure, using the potential method. 
\end{example}

We will show this using the potential function 
\[\Phi(D_i) = \sum_{u\in D_i}\log (u.size),\]
where $u.size$ is the number of nodes in the subtree rooted at $u$. Note that this sum only iterates over nodes which are included in at least one set $(u\in D_i)$. In particular, $\Phi(D_0) = 0$, so this is a valid potential function to use, since $\Phi(D_i)\geq 0 = \Phi(D_0)$ for all $i$. 

First, consider the $\textsc{Find-set}$ operation. If we call the operation on $v_0$, we will follow a path $v_0, v_1, \hdots, v_k = Rep[S(v_0)]$, so the actual cost is $k$. For each vertex in this path, we redirect its leader pointer to $v_k$. Therefore, for each $1\leq i\leq k-1$, the updated size of $v_i$ is $v_i.size-v_{i-1}.size$, since each node is losing all of the children of its child when it is redirected. So, our amortized cost is given by
\begin{align*}
    \hat{c_i}^{\textsc{Find-set}} = k + \Delta \Phi &= k + \sum_{i=1}^{k-1}\log\left(\frac{v_{i}.size - v_{i-1}.size}{v_{i}.size}\right) \\
    &= 1 + \sum_{i=1}^{k-1}\left[1+\log\left(\frac{v_{i}.size - v_{i-1}.size}{v_{i}.size}\right)\right].
\end{align*}
When $v_{i}.size > 2\cdot v_{i-1}.size$, the summand is non-negative and bounded above by $1+\log(1) = 1$. Otherwise, the summand is negative. However, along the chain, there can be at most $\log(n)$ doublings, since the total number of nodes is bounded above by $n$. Therefore, there are at most $\log(n)$ non-negative terms in the summand, each $\leq 1$, so $\hat{c_i}^{\textsc{Find-set}}\in O(\log n)$.  

Next, consider the $\textsc{Union}$ operation. This operation consists of two find-set operations, then a constant-time join operation, so the amortized cost is
\[\hat{c_i}^{\textsc{Union}} = 2\hat{c_i}^{\textsc{Find-set}} + c_{join} + \Delta \Phi_{join} = 2\hat{c_i}^{\textsc{Find-set}} + \Delta\Phi_{join}.\]
Since \textsc{Find-set} is $O(\log n)$ amortized, it suffices to compute $\Delta\Phi_{join}$:
\begin{align*}
    \Delta\Phi_{join} = \log\left(\frac{x.size + y.size}{x.size}\right) \leq \log\left(\frac{n}{x.size}\right)\in O(\log n).
\end{align*}
So, the amortized cost for $\textsc{Find-set}$ is also $O(\log n)$ as desired. 
