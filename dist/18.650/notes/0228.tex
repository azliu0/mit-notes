\section{February 28, 2023}

\subsection{p-values}

From last lecture: for a test $\Psi = \mathbbm{1}(\hat{\theta} > c)$, the level of $\Psi$ determines $c$. 

\begin{definition}
\deflabel

The (asymptotic) \ac{p-value} of a test $\Psi$ is the smallest (asymptotic) level $\alpha$ at which $\Psi$ rejects $H_0$.
\end{definition}

In general, when we conduct an $\alpha$-level test, we reject the null hypothesis if we find that the $p$-value is $\leq \alpha$. 

\begin{example}
\exlabel

Back to kissing experiment.
\end{example}
The level $(1-2\beta)$ confidence interval for the kissing interval was given by 
\[\mathcal{I} = \left[\ov{R}_n - \frac{q_{\beta}}{2\sqrt{n}}, \ov{R}_n + \frac{q_{\beta}}{2\sqrt{n}}\right]\]
The null hypothesis was that $p=1/2$, and the alternate hypothesis was that $p>1/2$.

To compute the $p$-value, we want to compute the first point at which $1/2$ fails to lie in this interval. Since we're looking for $p>1/2$, we equate the left part of the interval to $1/2$:
\[\frac{1}{2} = \ov{R}_n - \frac{q_{\beta}}{2\sqrt{n}}.\]
This gives
\[q_{\beta} = 2\sqrt{n}\left(\ov{R}_n - \frac{1}{2}\right),\]
from which we can solve for $\beta$ (recall that $\PP[\Norm(0,1) > q_{\beta}] = \beta$).

\subsection{Parametric Hypothesis Testing}

Start with a statistical model $(\Omega, (\PP_{\theta})_{\theta\in \Theta})$. 

\begin{example}
\exlabel

Wald's test
\end{example}

The test can come in any of the following forms: 
\begin{itemize}
    \item form 1: $H_0: \theta = \theta_0$, $H_1: \theta > \theta_0$
    \item form 2: $H_0: \theta = \theta_0$, $H_1: \theta < \theta_0$
    \item form 3: $H_0: \theta = \theta_0$, $H_1: \theta \neq \theta_0$
\end{itemize}

The test statistic is 
\[W = \frac{\hat{\theta}-\theta_0}{\sqrt{\widehat{\var}(\hat{\theta})}},\]
where $\widehat{\var}(\hat{\theta})$ is an \textit{estimator} of the variance of $\hat{\theta}$. For example, in the kissing example, we used $\sqrt{(1/2)\cdot (1-1/2)/n}$. 

Assuming that the test is level-$\alpha$, our tests are:
\begin{itemize}
    \item form 1: 
    \[\Psi = \mathbbm{1}(W > q_{\alpha}).\]
    When $W$ is normally distributed, the probability of a type $1$ error is $\alpha$, at the right tail of the distribution. For some $W_{obs}$, our $p$-value is 
    \[\PP[\Norm(0,1) > W_{obs}].\]
    \item form 2: 
    \[\Psi = \mathbbm{1}(W < -q_{\alpha}).\]
    When $W$ is normally distributed, the probability of a type $1$ error is $\alpha$, at the left tail of the distribution. For some $W_{obs}$, our $p$-value is 
    \[\PP[\Norm(0,1) < W_{obs}].\]
    \item form 3: 
    \[\Psi = \mathbbm{1}(\vert W\vert > q_{\alpha/2}).\]
    When $W$ is normally distributed, the probability of a type $1$ error is $\alpha/2 + \alpha/2 = \alpha$. For some $W_{obs}$, our $p$-value is 
    \[\PP[\vert \Norm(0,1)\vert > \vert W_{obs}\vert].\]
\end{itemize}

% \begin{example}
% \exlabel

% Students' t-test
% \end{example}


