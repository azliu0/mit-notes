\section{March 19, 2024}

\subsection{Continuous Information Theory}

Many of the formulas we used in the discrete case work mostly the same in the continuous case. Let $X,Y$ be continuous r.v.s; then, 
\[h(X) = -\int_{\infty}^{\infty} p_X(x) \log p_X(x)\ddd x,\] 
and 
\[h(X|Y=y) = -\int_{\infty}^{\infty}p_{X|Y}(x|y)\log p_{X|Y}(x|y)\ddd x,\]
and 
\[h(X|Y) = \int_{-\infty}^{\infty}p_Y(y)h(X|Y=y)\ddd y = -\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} p_{X,Y}(x,y)\log p_{X|Y}(x|y)\ddd x \ddd y.\] 
Mutual information is defined the same
\[I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X),\]
and information divergence as well
\[D(p||q) = \int_{-\infty}^{\infty}p(x) \log \frac{p(x)}{q(x)}\ddd x.\] 
There are some differences in the way that these functions behave under coordinate transformations. In the discrete cases, relabelling the alphabet does not change the behavior of any functions. In the continuous case, suppose that $X = g(S)$ for some monotonically increasing, differentiable mapping on $S$. Then for any $x$ and $s = g^{-1}(x)$, we must have:
\begin{itemize}
	\item $\ddd x = \ddd s \dot{g}(s)$
	\item $p_X(x)\dot{g}(s) = p_S(s)$. This isn't related to the main discussion, but there are a few ways to think about why this must be the case. The first is that we want the unit integral area to be the same should remain the same under transformation, i.e., $p_X(x) \ddd x = p_S(s) \ddd s$. Another way to think about this is that an interval of unit length $[s, s+\ddd s]$ gets mapped to $[g(s), g(s)+\dot{g}(s)\ddd s]$, which is longer by a factor of $\dot{g}(s)$; therefore, the height $p_X(x)$ should be shorter by a factor of $\dot{g}(s)$, in order to keep the total area $1$. 
\end{itemize}
So the differential entropy is
\begin{align*}
	h(X) = -\int_{\infty}^{\infty}\frac{p_S(s)}{\dot{g}(s)}\log \frac{p_S(s)}{\dot{g}(s)} \dot{g}(s)\ddd s = h(S) + \EE[\log(\dot{g}(s))].
\end{align*}
It is not invariant to coordinate transformations. On the other hand, mutual information under coordinate transformation is 
\begin{align*}
	I(X;Y) &= h(X) - h(X|Y) \\
				 &= h(S) + \EE[\log (\cdot{g}(s))] - h(S|Y) - \EE[\log (\cdot {g}(s))] \\
				 &= h(S) - h(S|Y),
\end{align*}
so it remains the same. Similarly, information divergence 
\begin{align*}
	D(p_x||q_x) &= \int_{-\infty}^{\infty}p_X(x)\log \frac{p_X(x)}{q_X(x)} \ddd x \\
					&= \int_{-\infty}^{\infty}p_S(s)\log \frac{p_S(s)}{q_S(s)} \ddd s = D(p_s||q_s) 
\end{align*}
is also invariant. 

\subsection{Gaussian Distribution Information Measures}

Now we derive some information measures on Gaussian distributions. Let $Y = aX + Z$, where $X\sim \Norm(0,1)$ and $Z\sim \Norm(0,1)$. This is the scalar version of the so-called \ac{innovations form} for Gaussian vectors. Then, we have that 
\begin{align*}
	p_{X|Y}(x|y) &\propto \exp \left(-\frac{1}{2}(y-ax)^2\right)\exp \left(-\frac{1}{2}x^2\right) \\
							 &\propto \exp \left(-\frac{1}{2} \left(x^2(a^2+1) - 2ayx + y^2\right)\right) \\
							 &\propto \exp \left(-\frac{1}{2} \left(\frac{x - ay/(a^2+1)}{1/(a+1)}\right)^2\right),
\end{align*}
so 
\[p_{X|Y}(x|y) \sim \Norm(\mu_{X|Y}, \lambda_{X|Y}) = \Norm\left(\frac{ay}{a^2+1}, \frac{1}{a+1}\right).\] 

\begin{example}
\exlabel

Differential entropy.
\end{example}

Let $X\sim \Norm(\mu, \sigma^2)$. Then,
\begin{align*}
	h(X) &= -\int_{-\infty}^{\infty}p_X(x)\log p_X(x) \ddd x\\
			 &= -\int_{-\infty}^{\infty}p_X(x)\left(-\frac{1}{2}\log(2\pi\sigma^2)-\frac{(x-\mu)^2}{2\sigma^2}\right)\ddd x \\
			 &= \frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2} \\
			 &= \frac{1}{2}\log(2\pi e\sigma^2).
\end{align*}

\begin{example}
\exlabel

Conditional differential entropy.
\end{example}

\noindent Consider the example $(X,Y)$ defined from before. From the conditional distribution, 
\[h(X|Y=y) = \frac{1}{2}\log\left(\frac{2\pi e}{a+1}\right).\] 
Therefore, 
\[h(X|Y) = \int_{-\infty}^{\infty}h(X|Y=y)\Norm(y;0,a^2+1)\ddd y = \frac{1}{2} \log \left(\frac{2\pi e}{a+1}\right).\] 

\begin{example}
\exlabel

Mutual information.
\end{example}

\noindent Using the same example $(X,Y)$ defined above,
\[I(X;Y) = h(X) - h(X|Y) = \frac{1}{2}\log(a+1).\] 

I am testing caching. 
