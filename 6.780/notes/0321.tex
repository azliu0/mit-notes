\section{March 19, 2024}

\subsection{Continuous Information Theory}

Many of the formulas we used in the discrete case work mostly the same in the continuous case. Let $X,Y$ be continuous r.v.s; then, 
\[h(X) = -\int_{\infty}^{\infty} p_X(x) \log p_X(x)\ddd x,\] 
and 
\[h(X|Y=y) = -\int_{\infty}^{\infty}p_{X|Y}(x|y)\log p_{X|Y}(x|y)\ddd x,\]
and 
\[h(X|Y) = \int_{-\infty}^{\infty}p_Y(y)h(X|Y=y)\ddd y = -\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} p_{X,Y}(x,y)\log p_{X|Y}(x|y)\ddd x \ddd y.\] 
Mutual information is defined the same
\[I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X),\]
and information divergence as well
\[D(p||q) = \int_{-\infty}^{\infty}p(x) \log \frac{p(x)}{q(x)}\ddd x.\] 
There are some differences in the way that these functions behave under coordinate transformations. In the discrete cases, relabelling the alphabet does not change the behavior of any functions. In the continuous case, suppose that $X = g(S)$ for some monotonically increasing, differentiable mapping on $S$. 

\subsection{Gaussian Distribution Information Measures}

Now we derive some information measures on Gaussian distributions. Let $Y = aX + Z$, where $X\sim \Norm(0,1)$ and $Z\sim \Norm(0,1)$. This is the scalar version of the so-called \ac{innovations form} for Gaussian vectors. Then, we have that 
\begin{align*}
	p_{X|Y}(x|y) &\propto \exp \left(-\frac{1}{2}(y-ax)^2\right)\exp \left(-\frac{1}{2}x^2\right) \\
							 &\propto \exp \left(-\frac{1}{2} \left(x^2(a^2+1) - 2ayx + y^2\right)\right) \\
							 &\propto \exp \left(-\frac{1}{2} \left(\frac{x - ay/(a^2+1)}{1/(a+1)}\right)^2\right),
\end{align*}
so 
\[p_{X|Y}(x|y) \sim \Norm(\mu_{X|Y}, \lambda_{X|Y}) = \Norm\left(\frac{ay}{a^2+1}, \frac{1}{a+1}\right).\] 

\begin{example}
\exlabel

Differential entropy.
\end{example}

Let $X\sim \Norm(\mu, \sigma^2)$. Then,
\begin{align*}
	h(X) &= -\int_{-\infty}^{\infty}p_X(x)\log p_X(x) \ddd x\\
			 &= -\int_{-\infty}^{\infty}p_X(x)\left(-\frac{1}{2}\log(2\pi\sigma^2)-\frac{(x-\mu)^2}{2\sigma^2}\right)\ddd x \\
			 &= \frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2} \\
			 &= \frac{1}{2}\log(2\pi e\sigma^2).
\end{align*}

\begin{example}
\exlabel

Conditional differential entropy.
\end{example}

\noindent Consider the example $(X,Y)$ defined from before. From the conditional distribution, 
\[h(X|Y=y) = \frac{1}{2}\log\left(\frac{2\pi e}{a+1}\right).\] 
Therefore, 
\[h(X|Y) = \int_{-\infty}^{\infty}h(X|Y=y)\Norm(y;0,a^2+1)\ddd y = \frac{1}{2} \log \left(\frac{2\pi e}{a+1}\right).\] 

\begin{example}
\exlabel

Mutual information.
\end{example}

\noindent Using the same example $(X,Y)$ defined above,
\[I(X;Y) = h(X) - h(X|Y) = \frac{1}{2}\log(a+1).\] 
