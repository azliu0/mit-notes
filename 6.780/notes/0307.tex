\section{March 7, 2024}

\subsection{Complete Data}

Consider the following setup. We have some observed data $y$ sampled from $Y$, which has distribution $p_Y(\cdot; x)$ for some $x\in \mathcal{X}$. We want 
\[\hat{x}(y) = \argmax_{a\in \mathcal{X}}\log p_Y(y; a),\] 
i.e., the maximum likelihood hypothesis that explains the data we observed. Define $\ell_Y(a;y) = \log p_Y(y;a)$, so that
\[\hat{x}(y) = \argmax_{a\in \mathcal{X}}\ell_Y(a; y). \] 
Now suppose that there exists some latents $z$ sampled from $Z$, which has distribution $p_Z(\cdot; x)$. $Z$ is the ``complete data'', i.e., $Y = g(Z)$ for some deterministic $g$. $Z$ always exists, but we may not be able to observe it; we have to guess a form that is reasonable and hope that the results work. Given the data, we can compute an expected likelihood of our latent distribution: 
\[\EE_{Z|Y}(\cdot | y; x')[\ell_Z(x; z)],\]
for any $x'\in \mathcal{X}$. We cannot compute the true value, since we are assuming that we don't have access to the complete data (in practice, the ``complete data'' is just a guess, e.g., I assume that the data is being generated from $k$-clusters). Also, since $Y=g(Z)$ deterministically, we can say that 
\[p_Z(z;x) = p_{Z,Y}(z,y;x) = p_{Z|Y}(z|y;x)p_Y(y;x),\]
so taking the log and then expectation of both sides gives 
\[\log p_Y(y;x) = \EE_{p_{Z|Y}(\cdot | y; x')}[\log p_Z(z;x)] - \EE_{p_{Z|Y}(\cdot | y; x')}[\log p_{Z|Y}(z|y; x)]\] 
for all $x,x'\in \mathcal{X}$. Re-write this term wise as 
\[\ell_Y(x;y) = U(x;x') + V(x;x').\] 
By Gibbs, $V(x;x')\geq V(x';x')$, so this rearranges to 
\begin{align*}
\ell_Y(x;y)\geq (U(x;x')-U(x';x')) + U(x';x')+V(x';x') \geq \Delta(x,x') + \ell_Y(x';y).
\end{align*} 
Given that we choose $x$ s.t. $\Delta(x,x') > 0$, we now have $\ell_Y(x;y) > \ell_Y(x';y)$, which gives a way to guarantee an increase in log likelihood. This is the foundation for the EM-algorithm.

\subsection{Expectation-Maximization Algorithm}

The EM-algorithm is as follows: 
\begin{itemize}
	\item Initialize $t=0$ and a guess for $\hat{x}^{(0)}$. 
	\item \textbf{E}: Compute 
		\[U(x;\hat{x}^{(t)}) = \EE_{p_{Z|Y}(\cdot | y; \hat{x}^{(t)})}[\log p_Z(z;x)].\] 
	\item \textbf{M}: Compute 
		\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}U(x; \hat{x}^{(t)}).\]
	\item Increment $t$ and repeat the \textbf{EM} cycle until convergence. 
\end{itemize}
The intuition behind the \textbf{M}-step here is to guarantee that $\Delta(\hat{x}^{(t+1)}, \hat{x}^{(t)}) \geq 0$, so that we have an increasing (non-decreasing) sequence of likelihoods 
\[\{\ell_Y(\hat{x}^{(t)})\}_{t\geq 0}.\] 
The hope is that after enough steps, we can converge on a optimal value. 

\subsection{EM on Logistic Regression}

\subsection{EM on Infectious Disease}

\subsection{EM Alternate Formulation}

Here we introduce an alternate formulation for the EM algorithm that we introduced earlier. Say the complete data is $Z\in \mathcal{Y}\times \mathcal{S}$, where $Y\in \mathcal{Y}$ is our observed data and $S\in \mathcal{S}$ is the latent. Consider the objective function 
\[\ln \tilde{p}_{Y}(y;x,q_{S|Y}(\cdot|y)) = \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y,S}(y,s;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)],\] 
for all $x\in \mathcal{X}$ and $y\in \mathcal{Y}$. This is similar to the original setup, but now the conditioning on $x$ no longer affects the distribution over the expected values, and moreover the distribution over the expected values is an additional input to our likelihood function. I'm not sure what the significance of these differences are yet, but this setup will supposedly be important when we learn about variational inference. 

Since we now have two inputs conditioning our likelihood function, we can alternate between them for our algorithm. First, guess $\hat{x}^{(0)}$, and then alternate
\[\hat{q}_{S|Y}^{(t)}(\cdot | y) = \argmax_{q\in \mathscr{P}_+^{\mathcal{S}}}\ln \tilde{p}_Y(y;\hat{x}^{(t)}, q_{S|Y}(\cdot | y)),\] 
\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}\ln \tilde{p}_Y(y;x,\hat{q}_{S|Y}^{(t)}(\cdot | y)).\] 
We will show that this is equivalent to the original EM formulation. First, note that 
\begin{align*}
	\ln \tilde{p}_{Y}(y;x,q_{S|Y}(\cdot|y)) &= \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y,S}(y,s;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)] \\
																					&= \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y}(y;x)] + \EE_{q_{S|Y}(\cdot | y)}[\ln p_{S|Y}(\cdot | y;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)] \\
																					&\leq \ln p_Y(y;x) + \EE_{q_{S|Y}(\cdot | y)}\ln q_{S|Y}(\cdot|y;x) - \EE_{q_{S|Y}}\ln q_{S|Y}(\cdot | y;x) = \ln p_Y(y;x), 
\end{align*}
by Gibbs. We have equality when $q_{S|Y}(\cdot | y) = p_{S|Y}(\cdot | y;x)$, so 
\[q^{(t)}_{S|Y}(\cdot | y) = p_{S|Y}(s|y;x^{(t)}).\] 
Now,
\begin{align*}
	\ln (\tilde{p}_y;x,q_{S|Y}^{(t)}(\cdot | y)) &= \EE_{p_{S|Y}(\cdot|y;x^{(t)})}[\ln p_{Y,S}(y,s;x)] - \EE_{p_{S|Y}(\cdot|y;x^{(t)})}[\ln p_{S|Y}(s|y)].
\end{align*}
The second term on the RHS is not a function of $x$, so it doesn't affect our $\argmax$. Since $Z=(Y,S)$, we thus have 
\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}\EE_{p_{Z|Y}(\cdot | y;x^{(t)})}[\ln p_{Z}(z;x)] = U(x;\hat{x}^{(t)}),\]
which recovers the original EM form, so we are done.


