\section{March 7, 2024}

\subsection{Complete Data}

Consider the following setup. We have some observed data $y$ sampled from $Y$, which has distribution $p_Y(\cdot; x)$ for some $x\in \mathcal{X}$. We want 
\[\hat{x}(y) = \argmax_{a\in \mathcal{X}}\log p_Y(y; a),\] 
i.e., the maximum likelihood hypothesis that explains the data we observed. Define $\ell_Y(a;y) = \log p_Y(y;a)$, so that
\[\hat{x}(y) = \argmax_{a\in \mathcal{X}}\ell_Y(a; y). \] 
Now suppose that there exists some latents $z$ sampled from $Z$, which has distribution $p_Z(\cdot; x)$. $Z$ is the ``complete data'', i.e., $Y = g(Z)$ for some deterministic $g$. $Z$ always exists, but we may not be able to observe it; we have to guess a form that is reasonable and hope that the results work. Given the data, we can compute an expected likelihood of our latent distribution: 
\[\EE_{Z|Y}(\cdot | y; x')[\ell_Z(x; z)],\]
for any $x'\in \mathcal{X}$. We cannot compute the true value, since we are assuming that we don't have access to the complete data (in practice, the ``complete data'' is just a guess, e.g., I assume that the data is being generated from $k$-clusters). Also, since $Y=g(Z)$ deterministically, we can say that 
\[p_Z(z;x) = p_{Z,Y}(z,y;x) = p_{Z|Y}(z|y;x)p_Y(y;x),\]
so taking the log and then expectation of both sides gives 
\[\log p_Y(y;x) = \EE_{p_{Z|Y}(\cdot | y; x')}[\log p_Z(z;x)] - \EE_{p_{Z|Y}(\cdot | y; x')}[\log p_{Z|Y}(z|y; x)]\] 
for all $x,x'\in \mathcal{X}$. Re-write this term wise as 
\[\ell_Y(x;y) = U(x;x') + V(x;x').\] 
By Gibbs, $V(x;x')\geq V(x';x')$, so this rearranges to 
\begin{align*}
\ell_Y(x;y)\geq (U(x;x')-U(x';x')) + U(x';x')+V(x';x') \geq \Delta(x,x') + \ell_Y(x';y).
\end{align*} 
Given that we choose $x$ s.t. $\Delta(x,x') > 0$, we now have $\ell_Y(x;y) > \ell_Y(x';y)$, which gives a way to guarantee an increase in log likelihood. This is the foundation for the EM-algorithm.

\subsection{Expectation-Maximization Algorithm}

The EM-algorithm is as follows: 
\begin{itemize}
	\item Initialize $t=0$ and a guess for $\hat{x}^{(0)}$. 
	\item \textbf{E}: Compute 
		\[U(x;\hat{x}^{(t)}) = \EE_{p_{Z|Y}(\cdot | y; \hat{x}^{(t)})}[\log p_Z(z;x)].\] 
	\item \textbf{M}: Compute 
		\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}U(x; \hat{x}^{(t)}).\]
	\item Increment $t$ and repeat the \textbf{EM} cycle until convergence. 
\end{itemize}
The intuition behind the \textbf{M}-step here is to guarantee that $\Delta(\hat{x}^{(t+1)}, \hat{x}^{(t)}) \geq 0$, so that we have an increasing (non-decreasing) sequence of likelihoods 
\[\{\ell_Y(\hat{x}^{(t)})\}_{t\geq 0}.\] 
The hope is that after enough steps, we can converge on a optimal value. 

\subsection{EM on Logistic Regression}

\subsection{EM on Infectious Disease}

Say we have a population of $N$ people over $T$ years, and data $y_i\in \{0,1\}$ signalling whether a subject $i\in [N]$ contracted an infectious disease by te end of the $T$ years. We have access to a record $\mu_{ij}\in \{0,1\}$ that tells us whether or not subject $i\in [N]$ was exposed to the disease in year $j\in [T]$. Our goal is to estimate the infection rate $r_j$ for each year $j\in [T]$. 

Naturally, the ``complete'' data that explains our observations can be represented by $z_{ij}\in \{0,1\}$ indicating whether $i$ contracted the disease during year $j$. We don't have access to this data, but we can use the EM algorithm on this latent space to estimate $r_j$. 

\V

\noindent We can show using a bit of casework that 
\[p_{z_{ij}}(z_{ij};r) = r_j^{\mu_{ij}z_{ij}}(1-r_j)^{\mu_{ij}z_{ij}},\] 
so 
\[\ln p_Z(z;r) = \sum_{i=1}^N\sum_{j=1}^T z_{ij}\mu_{ij}\ln r_j + \mu_{ij}(1-z_{ij})\ln (1-r_j).\] 
Now we perform $\mathbf{E}$: 
\begin{align*}
	&\EE_{p_{Z|Y}(\cdot | y;r')}\left[\sum_{i=1}^N\sum_{j=1}^Tz_{ij}\mu_{ij}\ln r_j + \mu_{ij}(1-z_{ij})\ln (1-r_j)\right] \\
	&\qquad\qquad= \sum_{j=1}^T\sum_{i=1}^N \mu_{ij}\EE_{p_{Z|Y}(\cdot | y;r')}[z_{ij}]\left(\frac{r_j}{1-r_j}\right) + \mu_{ij}\ln (1-r_j).
\end{align*}
We also need the expectation for each $z_{ij}$: 
\begin{align*}
	\EE_{p_{Z|Y}(\cdot | y;r')}[z_{ij}] = \PP[z_{ij}=1|Y=y,r'] = \frac{\mathbbm{1}(y_i=1)\mu_{ij}r_j'}{\sum_{j'=1}^T\mathbbm{1}(y_i=1)\mu_{ij'}r_{j'}'}.
\end{align*}
To perform $\mathbf{M}$, note that we can optimize one component at a time by taking the inner sum, i.e., 
\[r_j = \argmax_{r_j}\left(\sum_{i=1}^N\mu_{ij}\EE_{p_{Z|Y(\cdot | y;r')}}[z_{ij}]\left(\ln \frac{r_j}{1-r_j}\right) + \mu_{ij}\ln (1-r_j)\right).\] 
Taking critical points gives
\[\sum_{i=1}^N\left(\mu_{ij}\EE_{p_{Z|Y(\cdot | y;r')}}[z_{ij}]\cdot \frac{1-r_j}{r_j}\frac{(1-r_j) + r_j}{(1-r_j)^2} - \frac{\mu_{ij}}{1-r_j}\right) = 0,\] 
which gives
\[r_j = \frac{\sum_{i=1}^N\left(\mu_{ij}\cdot \frac{\mathbbm{1}(y_i=1)\mu_{ij}r_j'}{\sum_{j'=1}^T\mathbbm{1}(y_i=1)\mu_{ij'}r_{j'}'}\right)}{\sum_{i=1}^N\mu_{ij}}.\] 
Note that this result intuitively makes sense; amongst all the days where we observed a subject getting exposed to the disease ($\mu_{ij}=1$), we want $r_j$ to approach the true proportion $\EE[z_{ij}]$ of infected peoples. 

\subsection{EM Alternate Formulation}

Here we introduce an alternate formulation for the EM algorithm that we introduced earlier. Say the complete data is $Z\in \mathcal{Y}\times \mathcal{S}$, where $Y\in \mathcal{Y}$ is our observed data and $S\in \mathcal{S}$ is the latent. Consider the objective function 
\[\ln \tilde{p}_{Y}(y;x,q_{S|Y}(\cdot|y)) = \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y,S}(y,s;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)],\] 
for all $x\in \mathcal{X}$ and $y\in \mathcal{Y}$. This is similar to the original setup, but now the conditioning on $x$ no longer affects the distribution over the expected values, and moreover the distribution over the expected values is an additional input to our likelihood function. I'm not sure what the significance of these differences are yet, but this setup will supposedly be important when we learn about variational inference. 

Since we now have two inputs conditioning our likelihood function, we can alternate between them for our algorithm. First, guess $\hat{x}^{(0)}$, and then alternate
\[\hat{q}_{S|Y}^{(t)}(\cdot | y) = \argmax_{q\in \mathscr{P}_+^{\mathcal{S}}}\ln \tilde{p}_Y(y;\hat{x}^{(t)}, q_{S|Y}(\cdot | y)),\] 
\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}\ln \tilde{p}_Y(y;x,\hat{q}_{S|Y}^{(t)}(\cdot | y)).\] 
We will show that this is equivalent to the original EM formulation. First, note that 
\begin{align*}
	\ln \tilde{p}_{Y}(y;x,q_{S|Y}(\cdot|y)) &= \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y,S}(y,s;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)] \\
																					&= \EE_{q_{S|Y}(\cdot | y)}[\ln p_{Y}(y;x)] + \EE_{q_{S|Y}(\cdot | y)}[\ln p_{S|Y}(\cdot | y;x)] - \EE_{q_{S|Y}(\cdot | y)}[\ln q_{S|Y}(s|y)] \\
																					&\leq \ln p_Y(y;x) + \EE_{q_{S|Y}(\cdot | y)}\ln q_{S|Y}(\cdot|y;x) - \EE_{q_{S|Y}}\ln q_{S|Y}(\cdot | y;x) = \ln p_Y(y;x), 
\end{align*}
by Gibbs. We have equality when $q_{S|Y}(\cdot | y) = p_{S|Y}(\cdot | y;x)$, so 
\[q^{(t)}_{S|Y}(\cdot | y) = p_{S|Y}(s|y;x^{(t)}).\] 
Now,
\begin{align*}
	\ln (\tilde{p}_y;x,q_{S|Y}^{(t)}(\cdot | y)) &= \EE_{p_{S|Y}(\cdot|y;x^{(t)})}[\ln p_{Y,S}(y,s;x)] - \EE_{p_{S|Y}(\cdot|y;x^{(t)})}[\ln p_{S|Y}(s|y)].
\end{align*}
The second term on the RHS is not a function of $x$, so it doesn't affect our $\argmax$. Since $Z=(Y,S)$, we thus have 
\[\hat{x}^{(t+1)} = \argmax_{x\in \mathcal{X}}\EE_{p_{Z|Y}(\cdot | y;x^{(t)})}[\ln p_{Z}(z;x)] = U(x;\hat{x}^{(t)}),\]
which recovers the original EM form, so we are done.


