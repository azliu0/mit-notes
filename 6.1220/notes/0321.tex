\section{March 21, 2023}

\subsection{Linear Programming (LP)}

\begin{example}
\exlabel

Motivating example.
\end{example}

We are a politician with a limited budget. We can campaign on four issues: building roads (1), gun control (2), forming subsidies (3), and gasoline tax (4). We can campaign each of these issues to urban, suburban, or rural populations, at some cost for for each separate issue campaigned. 

For each additional dollar we spend campaigning a topic, this is the number of voters we gain from each region: 

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
     & u & s & r \\
\hline
    roads & -2 & 5 & 3 \\
\hline
    guns & 8 & 2 & -5\\
\hline 
    subsidies & 0 & 0 & 10\\
\hline
    gas & 10 & 0 & -2\\
\hline
\end{tabular}
\end{center}

Say $y_i$ is the amount of money that we spend on each topic. Say also that our political strategy insists that we get at least $50,000$ urban votes, $100,000$ suburban votes, and $25,000$ rural votes. 

Now, our problem is to find 
\[\min{(y_1+y_2+y_3+y_4)},\]
subject to the constraints 
\[\begin{cases}
-2y_1+8y_2+0y_3+10y_4\geq 50,000,\\
5y_1+2y_2+0y_3+0y_4\geq 100,000,\\
3y_1-5y_2+10y_3-2y_4\geq 25,000,\\
y_1, y_2, y_3, y_4\geq 0.
\end{cases}\]

The optimal solution $(y_1^*, y_2^*, y_3^*, y_4^*)=(2050000/111, 425000/111, 0, 625000/111)$, giving an optimal cost $3100000/111$.

\begin{definition}
\deflabel

\ac{Linear Programming} problems seek to maximize a \textbf{linear} objective subject to a \textbf{linear} constraint. 
\end{definition}

\begin{itemize}
    \item the variable vector is given by
    \[\Vec{x} = [x_1, \hdots, x_n]^T \in \RR^n.\]
    \item the objective function is given by 
    \[c_1x_1+\hdots + c_nx_n = \Vec{c}\cdot \Vec{x},\]
    where the vector of constants
    \[\Vec{c} = [c_1, \hdots, c_n]^T \in \RR^n.\]
    technically, the objective function is given by $\Vec{c}^T\cdot \Vec{x}$. for clarity, we won't write these transposes. 
    \item the constraints are given by 
    \[\sum_{i\in [n]}A_{ij}x_j\leq b_j\quad \forall j\in [m].\]
    more compactly, we will use the notation 
    \[A\cdot \Vec{x}\leq \Vec{b},\]
    with $A\in \RR^{m\times n}$, where $m$ is the number of constraints. 
\end{itemize}

\begin{definition}
\deflabel

Standard form LP.
\end{definition}

The ``standard form'' of linear programming problems is to maximize $\Vec{c}\cdot \Vec{x}$, subject to $A\Vec{x}\leq \Vec{b}$ and $\Vec{x}\geq 0$. As before, the ``$\leq$'' and ``$\geq$'' is row-wise, i.e., $\Vec{x}\geq 0\iff x_i\geq 0\forall i\in [n]$. 

\begin{theorem}
\claimlabel

Any LP can be reduced to standard form LP. 
\end{theorem}

\begin{proof}
If the objective is going the wrong direction, $\min{(\Vec{c}\cdot\Vec{x})}\iff \max{(-\Vec{c}\cdot \Vec{x})}$. Similar logic can be applied to any inequality that faces the wrong direction. If we insist that any constraint requires equality, we can turn the constraint into two separate constraints with both $\geq$ and $\leq$. For any $x_i\in \RR$, we can write $x_i=x_i^+ - x_i^-$, where $x_i^+, x_i^-\geq 0$. 
\end{proof}

\begin{example}
\exlabel

Maximize $x_1+x_2$ subject to $x_1+2x_2\leq 4$ and $x_1,x_2\geq 0$. 
\end{example}

$\Vec{c} = (1,1)$. One way to view this problem is to find the point in the triangular region bounded by the constraints that is ``as far as possible'' in the direction of $\Vec{c}$. 

We can reason that the optimal point must lie at one of the vertices as follows. The optimal solution must lie on a border, because for any point strictly inside of the triangle, we can move further in the direction of $\Vec{c}$ by moving to a boundary. Once we are on the boundary, by monotonicity, moving along at least one of the directions (right/left) on the boundary \textit{cannot decrease} the value of the objective function. So, it suffices to check the vertices of the triangle, giving is $(4,0)$ as the optimal solution.  

\subsection{LP Algorithms}

\begin{example}
\exlabel

Simplex (Dantzig 1947).
\end{example}

Like the previous example, walk from vertex to vertex along the feasible polytope in the direction of $\Vec{c}$. This is practical to implement, but has worst case exponential run time.

\begin{example}
\exlabel

Ellipsoid (Khachiyan 1979). 
\end{example}

Maintains an ellipsoid that is guaranteed to contain the optimal solution. At each step, the ellipsoid is shrinked. This achieves worst case polynomial runtime, but is impractical to implement. 

\begin{example}
\exlabel

Interior-point method (Karmarkar 1984).
\end{example}

Moves the current solution strategically so that it travels close to to the optimal solution. Instead of moving only on the boundaries (like the Simplex method), this moves the current point through the polytope itself. This is polynomial and practical to implement. 

\begin{misconception}
\warninglabel

If we force $x_1, \hdots, x_n\in \ZZ$, the question becomes NP-hard. Even though it seems related, it is much more difficult to algorithmically solve this variant of the problem. 
\end{misconception}

\subsection{LP Duality}

Let's return to the motivating example. It turns out that summing $25/222\cdot (1) + 46/222\cdot (2) + 14/222\cdot (3)$ gives
\[y_1+y_2+\frac{140}{222}y_3+y_4\geq \frac{31000000}{111}.\]
This immediately proves optimality, since the left hand side bounds our objective function from below. 

\begin{definition}
\deflabel

Dual LP
\end{definition}

We can take any LP in standard form (the ``primal program'') and transform it into an equivalent, dual LP (the ``dual program''). Take the primal program as it was written before. Then, the dual program seeks to minimize $\Vec{b}\cdot \Vec{y}$, subject to $A^T\Vec{y}\geq \Vec{c}$ and $\Vec{y}\geq 0$.

This can be thought of as a generalization of the way that we solved the motivating example:

\begin{example}
\exlabel


\end{example}

\begin{theorem}
\thmlabelname{Weak LP Duality}

For any feasible solution to the primal program $\Vec{x}$ and feasible solution to the dual program $\Vec{y}$, 
\[\Vec{b}\cdot \Vec{y}\geq \Vec{c}\cdot \Vec{x}.\]
\end{theorem}

\begin{proof}
Any solution to the primal program satisfies
\[\sum_j y_j\left(\sum_i A_{ij}x_i\right)\leq \sum_j y_jb_j = \Vec{b}\cdot \Vec{y},\]
since this is the primal constraints summed over all $y_i$. Similarly, any solution to the primal program satisfies
\[\sum_i x_i\left(\sum_j A_{ij}y_j\right)\geq \sum_i x_ic_i = \Vec{c}\cdot \Vec{x},\]
since this is the dual constraints summed over all $x_i$. On the other hand, the left hand side of both equations is the same: the first equation computes $y^TAx$, while the second computes $x^TA^Ty$. Therefore, $\Vec{b}\cdot \Vec{y}\geq \Vec{c}\cdot \Vec{x}$, as desired.
\end{proof}

\begin{theorem}
\thmlabelname{Strong LP duality}

If $x^*$ is optimal solution to the primal program and $y^*$ is the optimal solution to the dual program, then equality holds; 
\[\Vec{c}\cdot \Vec{x^*} = \Vec{b}\cdot \Vec{y^*}.\]
\end{theorem}

