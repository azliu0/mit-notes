\section{February 14, 2023}

Today we'll be going over randomized algorithms. Properties of randomized algorithms:
\begin{itemize}
    \item Decisions are in some way based on random numbers $r_1, \hdots, r_k\in_R \{1, \hdots, R\}$
    \item Given some input $x$, the same algorithm may run different sequences of operations, have different running times, and produce different outputs. 
\end{itemize}
Two examples of general classes of randomized algorithms:
\begin{itemize}
    \item \textbf{Monte Carlo} algorithms are always polynomial time. The probability that these algorithms return a correct answer is ``high'', but not necessarily $1$.
    \item \textbf{Las Vegas} algorithms have polynomial run time in expectation. On the other hand, it guarantees a correct answer. 
\end{itemize}

\subsection{Verifying matrix products}

Given matrices $A$,$B$,$C$, verify whether $A\cdot B = C$. For now, assume we are working modulo $2$. 
\hrulebar

Introducing \ac{Frievald's Algorithm}:
\begin{itemize}
    \item Pick a random binary $v = (v_1, \hdots, v_k)$, such that $\PP[v_i=1] = 1/2$
    \item Compute $\hat{v} = A(Bv)$ and $\overset{\sim}{v} = Cv$
    \item If $\hat{v} = \overset{\sim}{v}$, return YES. Otherwise, return NO. 
\end{itemize}
The runtime of this algorithm is $O(n^2)$, which is faster than normal matrix multiplication. This is an example of a Monte Carlo algorithm, since it is always time-efficient, but does not guarantee a correct answer. The main hope is that the randomization makes the probability that an incorrect answer is produced low. 

\begin{theorem}
\lemlabel

If $A\cdot B\neq C$, then $\PP[ABv\neq Cv]\geq 1/2$.
\end{theorem}

\begin{proof}
Let $D = C-AB \neq 0$. Consider any vector $r$ such that $Dr=0$, i.e., if our algorithm chooses $r$, then it would return an incorrect result. Since there exists some $D_{ij}=1$, we have $De_{i}\neq 0$, so $D(r+e_{i})\neq 0$. Note that $r+e_{i}$ is the same vector as $r$ with the $i$th bit flipped, so there is an injective mapping between vectors that return the incorrect result and vectors that return the correct result, which proves the lemma. 
\end{proof}

\subsection{Quick Select}

Recall the median finding (rank finding) algorithm we discussed last time: 
\begin{itemize}
    \item Pick an element $x\in S$ ``cleverly''
    \item Compute $L = \{y\in S : y < x\}$, and $G = \{y\in S : y > x\}$. From this, we know that rank$(x) = \vert L\vert + 1$. 
    \item If rank$(x) = i$, then we're done. If rank$(x) > i$, find the element of rank $i$ in $L$. Otherwise, find the element of rank $i-\vert L\vert - 1$ in $G$.
\end{itemize}
The main idea was that we could pick $x$ that was always $3/4$-balanced, to guarantee constant reduction for each subtask and consequently linear run time. If we were to remove this step and instead pick $x\in S$ randomly, we would still have a correct algorithm, but the run time would pick up a non-zero probability of not being linear.

The expected run time becomes:
\[T(n) = T\left(\frac{3}{4}n\right) + \left(\EE[\text{\# iterations}] + 1\right)\cdot cn,\]
where the expected number of iterations refers to the expected number of iterations it takes before $x$ is $3/4$-balanced (with respect to the most recent subtask for which $x$ was $3/4$-balanced). Since $\PP[X\text{ is }3/4-\text{balanced}]\geq 1/2$, $\EE[\text{\# iterations}]\leq 2$, so our expected runtime is still linear.  