\section{September 14, 2023}

\subsection{Last lecture review}

Defined $\per(x) = \gcd \{i : P^i(x,x)>0\}$. If $x\sim y$, then $\per(x) = \per(y)$. Period of irreducible $P$ is the period of any $x\in \mathcal{X}$.

We say $\pi$ is stationary if $\pi P = \pi$, which is the same as saying 
\[\sum_x \pi(x) P(x,y) = \pi(y),\]
which is the same as saying $X_0\sim \pi \implies X_1\sim \pi$. Here, $\sim$ means ``distributed as'', and not communication (slightly confusing).  

\begin{theorem}
\thmlabel

If $\mathcal{X}$ finite, there exists stationary distribution $\pi$.
\end{theorem}

\begin{theorem}
\thmlabel

If $P$ irreducible, $\vert \mathcal{X}\vert < \infty$, $\pi$ is unique. 
\end{theorem}

\begin{theorem}
\corlabel

\[\pi(x) = \frac{1}{\EE[\tau_x^+]}.\]
\end{theorem}

\subsection{Convergence Theorem}

\begin{definition}
\deflabel

$P$ is \ac{reversible} wrt $\mu$ if 
\[\mu(x)P(x,y) = \mu(y)P(y,x)\quad \forall x,y\in \mathcal{X}.\]
\end{definition}

\begin{theorem}
\proplabel

If $P$ is reversible wrt $\mu$, then $\mu P=\mu$, i.e., $\mu$ is stationary. 
\end{theorem}

Warning: the converse of this proposition is false.

\begin{example}
\exlabelname{Birth and death chain}

$\mathcal{X} = \{0,1,\hdots, n\}$. $P(x,y) = p_x$ if $y=x+1$, $P(x,y) = q_x$ if $y=x-1$, or $r_x$ if $y=x$. 
\end{example}

Assuming all probabilities positive, this is an irreducible markov chain. Let's try to find $\pi$ such that $P$ is reversible wrt $\pi$. 

Consider $\mu(0)P(0,y) = \mu(y)P(y,0)$. If $y=0$, both sides are the same; if $y>1$, both sides are $0$. So, we need $\mu(0)P(0,1)=\mu(y)P(1,0) \implies \mu(0)p_0=\mu(1)q_1$. In general, $\mu(i)p_i = \mu(i+1)q_{i+1}$, which implies that 
\[\mu(x) = \mu(x-1)\frac{p_{x-1}}{q_x} = \mu(0) \prod_{i=1}^x \frac{p_{i-1}}{q_i}.\]

So, the unique stationary distribution is
\[\pi(x) = \frac{\prod_{i=1}^x \frac{p_{i-1}}{q_i}}{\sum_x \prod_{i=1}^x \frac{p_{i-1}}{q_i}}\]

If $p_x=p\forall x$ and $q_x=q\forall x$, this simplifies: 
\[\pi(x) = \frac{(p/q)^x(1-p/q)}{1-(p/q)^{n+1}}\]

This also tells us that 
\[\EE(\tau_x^+) = \frac{1-(p/q)^n}{(p/q)^x(1-p/q)}. \]

Recall: for discrete random variables, we say $X_i\cd X$ if $\PP[X_i=x]\cinf \PP[X=x]$ (for continuous r.v.s we need to use the full cdf). Similarly, $X_i\rightarrow \pi$ if $\PP[X_i=x]\rightarrow \pi(x)$. 

\begin{theorem}
\thmlabelname{Convergence Theorem}

$P$ irreducible, aperiodic, $\vert \mathcal{X}\vert < \infty$. Then, we know there exists unique $\pi$ stationary, and 
\[X_i\cd \pi\]
for any starting distribution $X_0\sim \mu$. In other words, 
\[\lim_{i\rightarrow \infty}\PP[X_i=x|X_0\sim \mu] = \pi(x)\forall x.\]
\end{theorem}

We'll use the following proposition: 

\begin{theorem}
\proplabel

If $P$ irreducible, aperiodic, $\vert \mathcal{X}\vert < \infty$, then there exists $r>0$ s.t. $\forall i\geq r$, $P^i(x,y)>0$ for all $x,y\in \mathcal{X}$. 
\end{theorem}

(no proof)

Now we're ready for the main result. 
\begin{proof}
Claim: without loss of generality, we can take 
\[\mu = \delta_x,\]
where $\delta_x(y) = 1$ if $y=x$ and $0$ if $y\neq x$. In other words, it suffices to show that $P^i(x,y)\rightarrow \pi(y)\forall x,y$. We want $\mu P^i(y)\rightarrow \pi(y)\forall \mu, y$. This is the same as 
\[\lim_{i\rightarrow \infty}\mu P^i(y) = \sum_x\mu(x)\lim_{i\rightarrow \infty}P^i(x,y) = \pi(y),\]
which is true assuming that we show $P^i(x,y)\rightarrow \pi(y)$, call $(\star)$. 

Let $\Pi$ be a matrix whose rows are all $\pi$. We claim that $(\star)$ is equivalent to $P^i\rightarrow \Pi$. By the proposition, and the fact that the state space is finite, there exists $r$ and $0<\theta < 1$ such that 
\[P^r(x,y)\geq (1-\theta)\pi(y).\]

Let \[Q = \frac{1}{\theta}(P^r - (1-\theta)\Pi).\]

We claim that $Q$ is the transition matrix of a MC. $Q$ is a transition matrix because both $P^r$ and $\Pi$ are transition matrices, i.e., their rows sum to $1$, and therefore each row of $Q$ adds to $(1-(1-\theta)\cdot 1)/\theta = 1$. Also, we picked $\theta$ so that $Q(x,y)\geq 0$, so $Q$ is a transition matrix. 

Now, since $P^r = \theta Q + (1-\theta)\Pi$. Since $\theta < 1$, this means we always have non-zero chance of stepping towards the stationary distribution. Intuitively, this means that if we try hard enough, we'll eventually reach $\Pi$. More rigorously: 

\[p^{rK} = (1-\theta^k)\Pi + \theta^kQ^k,\]
which we can prove this with induction:
\begin{align*}
    P^{r(K+1)} &= P^{rK}P^r \\
    &= (1-\theta^K)\Pi P^r + \theta^{k}Q^kP^r \\
    &= (1-\theta^K)\Pi + \theta^kQ^k(\theta Q + (1-\theta)\Pi) \\
    &= (1-\theta^K)\Pi + \theta^{k+1}Q^{k+1} + (\theta^k-\theta^{k+1})Q^k\Pi.
\end{align*}

All that remains is to show that $Q^k\Pi = \Pi$. 
\[Q^k\Pi(x,y) = \sum_z Q^k(x,z)\Pi(z,y) = \sum_z Q^k(x,z)\pi(y) = \pi(y), \]
thus 
\[P^{r(K+1)} = (1-\theta^{K+1})\Pi + \theta^{K+1}Q^{K+1},\]
which completes the induction.

Finally, taking the limit, we have $\lim_{k\rightarrow \infty}P^{rK} = \Pi$. In general, 
\[\lim_{n\rightarrow \infty}P^n = \lim_{n\rightarrow \infty}P^{r\lfloor n/r\rfloor + (n - r\lfloor n/r\rfloor)} = \Pi. \]
\end{proof}