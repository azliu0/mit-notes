\section{February 8, 2024}

\subsection{Tail Bounds}

Some important tail bounds that we'll use in this class.

\begin{theorem}
\thmlabelname{Markov's Inequality}

\[\PP[X \geq t]\leq \frac{\EE[X]}{t}, \quad t > 0.\] 
\end{theorem}

\begin{theorem}
\thmlabelname{Chebyshev's Inequality}

For any real-valued r.v. $X$ with mean $\mu$, 
\[\PP[\vert X-\mu\vert \geq t]\leq \frac{\sigma^2}{t^2}.\] 
\end{theorem}

Some useful applications of Markov's inequality: 
\begin{itemize}
	\item higher moments: 
		\[\PP[\vert X-\mu\vert\geq t] = \PP[\vert X-\mu\vert^p \geq t^p] \leq \min_{p\geq 1}\frac{\EE[\vert X-\mu\vert]^p}{t^p}\] 
	\item exponentiated r.v.s: 
	\[\PP[X - \mu\geq t] = \PP[e^{\lambda(X-\mu)}\geq e^{\lambda t}] \leq \inf_{\lambda > 0}e^{-t\lambda} \EE[e^{\lambda(X-\mu)}].\] 
\end{itemize}
The second expression shows us that deducing tail bounds for means is intimately related to better understanding \ac{moment generating functions} (MGFs), i.e., 
\[\text{MGF}_X(\lambda) = \EE[e^{\lambda X}].\] 

\subsection{Sub-Gaussian Random Variables}

\begin{definition}
\deflabel

A random variable $X$ with mean $\mu = \EE[X]$ is \ac{$\sigma$-sub-Gaussian} if $\EE[e^{\lambda(X - \mu)}] \leq e^{\lambda^2\sigma^2 / 2}$ for all $\lambda\in \RR$. 
\end{definition}

We can show that this holds when $X\sim \Norm(\mu, \sigma^2)$, hence motivating the definition, by directly deriving the MGF for $X$.
\begin{align*}
	\text{MGF}_X(\lambda) = \EE[e^{\lambda X}] &= \frac{1}{\sigma \sqrt{2\pi}}\int_{-\infty}^{\infty}e^{\lambda x}e^{-\frac{1}{2}\left(\frac{(x-\mu)^2}{\sigma^2}\right)}\ddd x \\
																						 &= \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}\exp\left[-\frac{1}{2\sigma^2}\left(x^2-2\mu x+\mu^2 - 2\sigma^2\lambda x\right)\right] \ddd x\\
																						 &= \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}\exp\left[-\frac{1}{2\sigma^2}\left(x - (\mu + \sigma^2\lambda)\right)^2 - 2\mu\sigma^2\lambda - \sigma^4\lambda^2\right]\ddd x \\
																						 &= e^{\mu\lambda + \sigma^2\lambda^2/2}.
\end{align*}
The key is the quadratic exponential tail decay; this is generally what we use to characterize Gaussian / sub-Gaussian behavior. 

\begin{theorem}
	\claimlabelname{Bounded r.v.s are sub-Gaussian}

Given r.v. $X\in [a,b]$, $\EE[X] = \mu$. Then, $X$ is sub-Gaussian with $\sigma=(b-a)$.
\end{theorem}

It turns out that we can also show $\sigma = (b-a)/2$, but we won't show this during lecture, since the technique used to show the weaker result is more interesting.

\begin{proof}
Let $\tilde{X}$ be i.i.d. to $X$. Then,
\begin{align*}
	\EE_X[e^{\lambda(X-\mu)}] &= \EE_X[e^{\lambda(X - \EE_{\tilde{X}}[\tilde{X}])}] \\
														&\leq \EE_{X, \tilde{X}}[e^{\lambda(X-\tilde{X})}],
\end{align*}
by Jensen's inequality. Since $X,\tilde{X}$ are i.i.d., $(X-\tilde{X})$ has a distribution symmetric around $0$. Now, we also have that
\[(X-\tilde{X}) \overset{\text{dist}}{=} \varepsilon(X-\tilde{X}),\] 
where $\varepsilon \in \{\pm 1\}$ with equal probability (also called a \ac{Rademacher} random variable). Therefore, 
\begin{align*}
	\EE_{\tilde{X}}[e^{\lambda(X-\mu)}]&\leq \EE_{\tilde{X},X}[\EE_{\varepsilon}[e^{\lambda\varepsilon(X-\tilde{X})}]] \\
																		 &\leq \EE_{X, \tilde{X}}[e^{\lambda^2(X-\tilde{X})^2/2}],
\end{align*}
since $\varepsilon$ is $1$-sub-gaussian. Finally, since $X$ is bounded, 
\begin{align*}
	\EE_{X, \tilde{X}}[e^{\lambda^2(X-\tilde{X})^2/2}] \leq e^{\lambda^2(b-a)^2/2}.
\end{align*}
\end{proof}

\subsection{More on sub-Gaussians}

\begin{definition}
\deflabelname{Addition property of Gaussians}

Given $X_1\sim \Norm(0,\sigma_1^2)$ and $X_2\sim \Norm(0,\sigma_2^2)$, 
\[X_1+X_2\sim \Norm(0, \sigma_1^2+\sigma_2^2).\] 
\end{definition}

\begin{theorem}
\claimlabelname{Addition property of sub-Gaussians}

Given $X_i\sim \sigma_i$-sub-Gaussian, $i\in \{1,2\}$, then $X_1+X_2$ is $\sqrt{\sigma_1^2+\sigma_2^2}$-sub-Gaussian. 
\end{theorem}

\begin{proof}
\begin{align*}
	\EE_{X_1,X_2}[e^{\lambda(X_1+X_2)}] &= \EE_{X_1,X_2}[e^{\lambda X_1}e^{\lambda X_2}] \\
																			&= \EE_{X_1}[e^{\lambda X_1}]\EE_{X_2}[e^{\lambda X_2}] \\
																			&\leq e^{\lambda^2\sigma_1^2/2}e^{\lambda^2\sigma_2^2/2} = e^{\lambda^2(\sigma_1^2+\sigma_2^2)/2}.
\end{align*}
\end{proof}

A consequence of this fact is that given $X_i\sim \sigma$-sub-Gaussian, i.i.d. with zero-mean, then 

\[\frac{1}{\sqrt{n}}\sum_{i=1}^n X_i\sim \sigma\text{-sub-Gaussian},\] 
or equivalently 
\[\frac{1}{n}\sum_{i=1}^n X_i\sim \frac{\sigma}{\sqrt{n}}\text{-sub-Gaussian}.\] 

\begin{example}
\exlabelname{Survey sampling}

Two candidates for an election $A$ and $B$.
\end{example}

Sample people $i=1, \hdots, n$, giving responses $X_i = 1$ if $A$ or $0$ if $B$. Let $\mu^*$ be the actual fraction of people who will vote $A$. Let $\hat{\mu}$ be our estimator for $\mu^*$: 

\[\hat{\mu} = \sum_{i=1}^n X_i.\] 

We can construct a confidence interval $\hat{\mathcal{I}}$ for our estimator, and we would like to know at what point we can say

\[\PP[\hat{\mathcal{I}}\ni \mu^{*}] \geq 1-\delta.\] 
For example, with $\delta=0.02$, interval width of $0.03$, we require $n\approx 10000$ to make this guarantee.  

We can model $X_i\sim \Bern(\mu^*)$. Since $X_i\in [0,1]$, our earlier result shows that $X_i$ is $1/2$-sub-Gaussian. Using additivity and i.i.d., our sample mean $\hat{\mu}$ is $1/(2\sqrt{n})$-sub-Gaussian. Thus, 

\[\EE[e^{\lambda(\hat{\mu}-\mu^*)}] \leq e^{\lambda^2/2\cdot 1/(4n)} = e^{\lambda^2/(8n)}.\]
Using Chernoff,

\[\PP[\vert \hat{\mu}-\mu^*\vert \geq s] \leq 2e^{2ns^2},\] 
for some $s > 0$. So for some fixed $\delta$, we can make a guarantee about interval width $s = \sqrt{\log(2/\delta) / (2n)}$. 

\begin{theorem}
\lemlabelname{Hoeffding's Lemma}

For any zero-mean r.v. X with values in $[a,b]$, the MGF satisfies 
\[\EE[e^{\lambda X}] \leq e^{\lambda^2 (b-a)^2 / 8}.\]
for all $\lambda$. 
\end{theorem}

The following proof is taken from \href{https://courses.cs.washington.edu/courses/cse521/21au/521-lecture-3.pdf}{these lecture notes}.

\begin{proof}
Since $e^{sx}$ is convex, 
\[e^{sX}\leq \frac{b-X}{b-a}e^{sa} + \frac{X-a}{b-a}e^{sb},\] 
so 
\[\EE[e^{sX}]\leq \frac{b-\EE[X]}{b-a}e^{sa} + \frac{\EE[X]-a}{b-a}e^{sb} = \frac{b}{b-a}e^{sa}-\frac{a}{b-a}e^{sb}.\] 
Make the substitution $p = -a/(b-a)$ so that the above expression simplifies to
\[(1-p+pe^{s(b-a)})e^{-sp(b-a)},\] 
and again substitute $u=s(b-a)$ so that it further simplifies to
\[\varphi(u) := (1-p+pe^u)e^{pu}.\]
Now we can bound $\varphi(u)$. Taking derivatives, 
\begin{align*}
	\varphi'(u) := -p+\frac{pe^u}{1-p+pe^u} \\
	\varphi''(u) := \frac{p(1-p)e^u}{(1-p+pe^u)^2}. 
\end{align*}
By Taylor's theorem (see \href{https://math.stackexchange.com/questions/3238111/application-of-taylors-theorem-find-upper-bound-for-remainder}{here}), we have for some $z\in [0,u]$
\[\varphi(u) = \phi(0) + u\phi'(0) + \frac{1}{2}u^2\phi''(z)\leq \phi(0) + u\phi'(0) + \sup_z \frac{1}{2}u^2\phi''(z),\]
so substituting in the expressions from above 
\[\varphi(u) \leq \sup_z \frac{1}{2}u^2 \varphi''(z).\] 
Bashing critical points eventually gives the upper bound $1/4$, from which we get 
\[\EE[e^{sX}]\leq e^{\varphi(u)} \leq e^{u^2/8} \leq e^{s^2(b-a)^2/8},\]
as desired.
\end{proof}

\begin{theorem}
\thmlabelname{Hoeffding's Inequality}

Let $X_1,\hdots,X_m$ be r.v. with $\EE[X_i]=\mu_i$, $a_i\leq X_i\leq b_i$, and independent. Then, 
\[\PP\left[\sum_{i=1}^m(X_i-\mu_i)\geq t\right]\leq e^{-2t^2m^2/(\sum_i (b_i-a_i)^2)}.\] 
\end{theorem}

\begin{proof}
	Define $Z_i = X_i-\EE[X_i]$, so that $\EE[Z_i] = 0$. By Chernoff, for any $s>0$, we have 
	\[\PP[\sum_i Z_i\geq t] = \PP[\exp\left(s\sum_i Z_i\right)\geq e^{st}] \leq \frac{\EE[\prod_{i=1}^m e^{sZ_i}]}{e^{st}}.\] 
	Since $Z_i$ are independent, we can move the expectation inside of the product. Applying the Hoeffding Lemma then gives
	\[\frac{\EE[\prod_i e^{sZ_i}]}{e^{st}} = \frac{\prod_i \EE[e^{sZ_i}]}{e^{st}}\leq \exp\left(-st + \frac{s^2}{8}\sum_i (b_i-a_i)^2.\right)\] 
	Substituting $s = \frac{4t}{\sum_i (b_i-a_i)^2}$ gives the result.
\end{proof}
\begin{example}
\exlabel

$X\sim \Norm(0,1)$ is $1$-sub-Gaussian. Let $Y=X^2$ is not sub-Gaussian. 
\end{example}
First compute the MGF:
\begin{align*}
	M_{X}(\lambda) = \EE[e^{\lambda X^2}] = \frac{1}{\sqrt{2\pi}}\int e^{\lambda z^2}e^{-(z-1)^2/2}\ddd z = \frac{1}{\sqrt{1-\lambda}}
\end{align*}
for $\lambda\in (0,1)$. Despite not being sub-Gaussian, it is close, which we will show in more detail next time. 


\begin{example}
\exlabel

Consider an $n$-dimensional Gaussian, $X = (X_1, \hdots, X_n)$, where each $X_i\sim \Norm(0,1)$. 
\end{example}
Then $\EE[\lVert X\rVert_2^2 / n] = \EE[\sum X_i^2 / n] = 1$, it turns out that 
\[\PP\left[ \left\vert\frac{\lVert X\rVert^2}{n^2} - 1\right\vert \geq \delta\right] \leq 2e^{-cn\delta^2}\] 
holds for all $\delta\in (0,1)$. This looks very similar to the sub-gaussian tail bound from earlier, but will only hold for small delta. For larger delta, the tail bound becomes linear in $\delta$, i.e., exponential. This is example is just to provide some intuition on tail bounds. We will show this in more detail in the next lecture. 

